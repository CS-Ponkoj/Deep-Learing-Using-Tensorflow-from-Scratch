{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install tensorflow\n",
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "# check version\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 step life cycle of a deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Define the model.                  \n",
    "2.Compile the model.                    \n",
    "3.Fit the model.                 \n",
    "4.Evaluate the model.                  \n",
    "5.Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Defining the model                        \n",
    "requires that you first select the type of model that you need and then choose the architecture or network topology. This involves defining the layers of the model, configuring each layer with a number of nodes and activation function, and connecting the layers together into a cohesive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Compiling_Model            (model.compile())                       \n",
    "requires that you first select a loss function that you want to optimize, such as mean squared error or cross-entropy.          \n",
    "It also requires that you select an algorithm to perform the optimization procedure, typically stochastic gradient descent, or a modern variation, such as Adam. It may also require that you select any performance metrics to keep track of during the model training process.        \n",
    "This involves calling a function to compile the model with the chosen configuration, which will prepare the appropriate data structures required for the efficient use of the model you have defined.  \n",
    "The optimizer can be specified as a string for a known optimizer class, e.g. ‘sgd‘ for stochastic gradient descent, or you can configure an instance of an optimizer class and use that.           \n",
    "\n",
    "The three most common loss functions are:           \n",
    "1.‘binary_crossentropy‘ for binary classification.       \n",
    "2.‘sparse_categorical_crossentropy‘ for multi-class classification.            \n",
    "3.‘mse‘ (mean squared error) for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Fit the Model  (model.fit())                                        \n",
    "Fitting the model requires that you first select the training configuration, such as the number of epochs (loops through the training dataset) and the batch size (number of samples in an epoch used to estimate model error).       \n",
    "Training applies the chosen optimization algorithm to minimize the chosen loss function and updates the model using the backpropagation of error algorithm.           \n",
    "While fitting the model, a progress bar will summarize the status of each epoch and the overall training process. This can be simplified to a simple report of model performance each epoch by setting the “verbose” argument to 2. All output can be turned off during training by setting “verbose” to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Evaluate the Model  (model.evaluate())                          \n",
    "Evaluating the model requires that you first choose a holdout dataset used to evaluate the model. This should be data not used in the training process so that we can get an unbiased estimate of the performance of the model when making predictions on new data.\n",
    "The speed of model evaluation is proportional to the amount of data you want to use for the evaluation, although it is much faster than training as the model is not changed.\n",
    "From an API perspective, this involves calling a function with the holdout dataset and getting a loss and perhaps other metrics that can be reported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Make a Prediction (model.predict())               \n",
    "It requires you have new data for which a prediction is required, e.g. where you do not have the target values.\n",
    "From an API perspective, you simply call a function to make a prediction of a class label, probability, or numerical value: whatever you designed your model to predict.\n",
    "You may want to save the model and later load it to make predictions. You may also choose to fit a model on all of the available data before you start using it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Model API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this involves defining a Sequential class and adding layers to the model one by one in a linear manner, from input to output.             \n",
    "The example below defines a Sequential MLP model that accepts eight inputs, has one hidden layer with 10 nodes and then an output layer with one node to predict a numerical value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a model defined with the sequential api\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(8,))) #hidden layer\n",
    "model.add(Dense(1)) #output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential API is easy to use because you keep calling model.add() until you have added all of your layers.\n",
    "\n",
    "For example, here is a deep MLP with five hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a model defined with the sequential api\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(8,))) #1st hidden layer\n",
    "model.add(Dense(80))                    #2nd hidden layer   \n",
    "model.add(Dense(30))                    #3rd hidden layer \n",
    "model.add(Dense(10))                    #4th hidden layer  \n",
    "model.add(Dense(5))                     #5th hidden layer\n",
    "model.add(Dense(1))                     #output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Model API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It involves explicitly connecting the output of one layer to the input of another layer. Each connection is specified.\n",
    "\n",
    "First, an input layer must be defined via the Input class, and the shape of an input sample is specified. We must retain a reference to the input layer when defining the model.                  \n",
    "Next, a fully connected layer can be connected to the input by calling the layer and passing the input layer. This will return a reference to the output connection in this new layer.We can then connect this to an output layer in the same manner.              \n",
    "Once connected, we define a Model object and specify the input and output layers.                  \n",
    "The complete example is listed below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a model defined with the functional api\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "# define the layers\n",
    "x_in = Input(shape=(8,))       #input layer\n",
    "x = Dense(10)(x_in)            #hidden layer   \n",
    "x_out = Dense(1)(x)            #output layer\n",
    "# define the model\n",
    "model = Model(inputs=x_in, outputs=x_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP for Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Multilayer Perceptron model, or MLP for short, is a standard fully connected neural network model. It is comprised of layers of nodes where each node is connected to all outputs from the previous layer and the output of each node is connected to all inputs for nodes in the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Lonosphere Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99539</td>\n",
       "      <td>-0.05889</td>\n",
       "      <td>0.85243</td>\n",
       "      <td>0.02306</td>\n",
       "      <td>0.83398</td>\n",
       "      <td>-0.37708</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.03760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51171</td>\n",
       "      <td>0.41078</td>\n",
       "      <td>-0.46168</td>\n",
       "      <td>0.21266</td>\n",
       "      <td>-0.34090</td>\n",
       "      <td>0.42267</td>\n",
       "      <td>-0.54487</td>\n",
       "      <td>0.18641</td>\n",
       "      <td>-0.45300</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.18829</td>\n",
       "      <td>0.93035</td>\n",
       "      <td>-0.36156</td>\n",
       "      <td>-0.10868</td>\n",
       "      <td>-0.93597</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.04549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26569</td>\n",
       "      <td>-0.20468</td>\n",
       "      <td>-0.18401</td>\n",
       "      <td>-0.19040</td>\n",
       "      <td>-0.11593</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.06288</td>\n",
       "      <td>-0.13738</td>\n",
       "      <td>-0.02447</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.03365</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.12062</td>\n",
       "      <td>0.88965</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.40220</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>-0.22145</td>\n",
       "      <td>0.43100</td>\n",
       "      <td>-0.17365</td>\n",
       "      <td>0.60436</td>\n",
       "      <td>-0.24180</td>\n",
       "      <td>0.56045</td>\n",
       "      <td>-0.38238</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.45161</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.71216</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90695</td>\n",
       "      <td>0.51613</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.20099</td>\n",
       "      <td>0.25682</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.32382</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.02401</td>\n",
       "      <td>0.94140</td>\n",
       "      <td>0.06531</td>\n",
       "      <td>0.92106</td>\n",
       "      <td>-0.23255</td>\n",
       "      <td>0.77152</td>\n",
       "      <td>-0.16399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65158</td>\n",
       "      <td>0.13290</td>\n",
       "      <td>-0.53206</td>\n",
       "      <td>0.02431</td>\n",
       "      <td>-0.62197</td>\n",
       "      <td>-0.05707</td>\n",
       "      <td>-0.59573</td>\n",
       "      <td>-0.04608</td>\n",
       "      <td>-0.65697</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83508</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.73739</td>\n",
       "      <td>-0.14706</td>\n",
       "      <td>0.84349</td>\n",
       "      <td>-0.05567</td>\n",
       "      <td>0.90441</td>\n",
       "      <td>-0.04622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04202</td>\n",
       "      <td>0.83479</td>\n",
       "      <td>0.00123</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.12815</td>\n",
       "      <td>0.86660</td>\n",
       "      <td>-0.10714</td>\n",
       "      <td>0.90546</td>\n",
       "      <td>-0.04307</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95113</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>0.95183</td>\n",
       "      <td>-0.02723</td>\n",
       "      <td>0.93438</td>\n",
       "      <td>-0.01920</td>\n",
       "      <td>0.94590</td>\n",
       "      <td>0.01606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01361</td>\n",
       "      <td>0.93522</td>\n",
       "      <td>0.04925</td>\n",
       "      <td>0.93159</td>\n",
       "      <td>0.08168</td>\n",
       "      <td>0.94066</td>\n",
       "      <td>-0.00035</td>\n",
       "      <td>0.91483</td>\n",
       "      <td>0.04712</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.94701</td>\n",
       "      <td>-0.00034</td>\n",
       "      <td>0.93207</td>\n",
       "      <td>-0.03227</td>\n",
       "      <td>0.95177</td>\n",
       "      <td>-0.03431</td>\n",
       "      <td>0.95584</td>\n",
       "      <td>0.02446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03193</td>\n",
       "      <td>0.92489</td>\n",
       "      <td>0.02542</td>\n",
       "      <td>0.92120</td>\n",
       "      <td>0.02242</td>\n",
       "      <td>0.92459</td>\n",
       "      <td>0.00442</td>\n",
       "      <td>0.92697</td>\n",
       "      <td>-0.00577</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90608</td>\n",
       "      <td>-0.01657</td>\n",
       "      <td>0.98122</td>\n",
       "      <td>-0.01989</td>\n",
       "      <td>0.95691</td>\n",
       "      <td>-0.03646</td>\n",
       "      <td>0.85746</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02099</td>\n",
       "      <td>0.89147</td>\n",
       "      <td>-0.07760</td>\n",
       "      <td>0.82983</td>\n",
       "      <td>-0.17238</td>\n",
       "      <td>0.96022</td>\n",
       "      <td>-0.03757</td>\n",
       "      <td>0.87403</td>\n",
       "      <td>-0.16243</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84710</td>\n",
       "      <td>0.13533</td>\n",
       "      <td>0.73638</td>\n",
       "      <td>-0.06151</td>\n",
       "      <td>0.87873</td>\n",
       "      <td>0.08260</td>\n",
       "      <td>0.88928</td>\n",
       "      <td>-0.09139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.15114</td>\n",
       "      <td>0.81147</td>\n",
       "      <td>-0.04822</td>\n",
       "      <td>0.78207</td>\n",
       "      <td>-0.00703</td>\n",
       "      <td>0.75747</td>\n",
       "      <td>-0.06678</td>\n",
       "      <td>0.85764</td>\n",
       "      <td>-0.06151</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1        2        3        4        5        6        7        8   \\\n",
       "0     1   0  0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708  1.00000   \n",
       "1     1   0  1.00000 -0.18829  0.93035 -0.36156 -0.10868 -0.93597  1.00000   \n",
       "2     1   0  1.00000 -0.03365  1.00000  0.00485  1.00000 -0.12062  0.88965   \n",
       "3     1   0  1.00000 -0.45161  1.00000  1.00000  0.71216 -1.00000  0.00000   \n",
       "4     1   0  1.00000 -0.02401  0.94140  0.06531  0.92106 -0.23255  0.77152   \n",
       "..   ..  ..      ...      ...      ...      ...      ...      ...      ...   \n",
       "346   1   0  0.83508  0.08298  0.73739 -0.14706  0.84349 -0.05567  0.90441   \n",
       "347   1   0  0.95113  0.00419  0.95183 -0.02723  0.93438 -0.01920  0.94590   \n",
       "348   1   0  0.94701 -0.00034  0.93207 -0.03227  0.95177 -0.03431  0.95584   \n",
       "349   1   0  0.90608 -0.01657  0.98122 -0.01989  0.95691 -0.03646  0.85746   \n",
       "350   1   0  0.84710  0.13533  0.73638 -0.06151  0.87873  0.08260  0.88928   \n",
       "\n",
       "          9   ...       25       26       27       28       29       30  \\\n",
       "0    0.03760  ... -0.51171  0.41078 -0.46168  0.21266 -0.34090  0.42267   \n",
       "1   -0.04549  ... -0.26569 -0.20468 -0.18401 -0.19040 -0.11593 -0.16626   \n",
       "2    0.01198  ... -0.40220  0.58984 -0.22145  0.43100 -0.17365  0.60436   \n",
       "3    0.00000  ...  0.90695  0.51613  1.00000  1.00000 -0.20099  0.25682   \n",
       "4   -0.16399  ... -0.65158  0.13290 -0.53206  0.02431 -0.62197 -0.05707   \n",
       "..       ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "346 -0.04622  ... -0.04202  0.83479  0.00123  1.00000  0.12815  0.86660   \n",
       "347  0.01606  ...  0.01361  0.93522  0.04925  0.93159  0.08168  0.94066   \n",
       "348  0.02446  ...  0.03193  0.92489  0.02542  0.92120  0.02242  0.92459   \n",
       "349  0.00110  ... -0.02099  0.89147 -0.07760  0.82983 -0.17238  0.96022   \n",
       "350 -0.09139  ... -0.15114  0.81147 -0.04822  0.78207 -0.00703  0.75747   \n",
       "\n",
       "          31       32       33  34  \n",
       "0   -0.54487  0.18641 -0.45300   g  \n",
       "1   -0.06288 -0.13738 -0.02447   b  \n",
       "2   -0.24180  0.56045 -0.38238   g  \n",
       "3    1.00000 -0.32382  1.00000   b  \n",
       "4   -0.59573 -0.04608 -0.65697   g  \n",
       "..       ...      ...      ...  ..  \n",
       "346 -0.10714  0.90546 -0.04307   g  \n",
       "347 -0.00035  0.91483  0.04712   g  \n",
       "348  0.00442  0.92697 -0.00577   g  \n",
       "349 -0.03757  0.87403 -0.16243   g  \n",
       "350 -0.06678  0.85764 -0.06151   g  \n",
       "\n",
       "[351 rows x 35 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
    "df = read_csv(path, header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351 entries, 0 to 350\n",
      "Data columns (total 35 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       351 non-null    int64  \n",
      " 1   1       351 non-null    int64  \n",
      " 2   2       351 non-null    float64\n",
      " 3   3       351 non-null    float64\n",
      " 4   4       351 non-null    float64\n",
      " 5   5       351 non-null    float64\n",
      " 6   6       351 non-null    float64\n",
      " 7   7       351 non-null    float64\n",
      " 8   8       351 non-null    float64\n",
      " 9   9       351 non-null    float64\n",
      " 10  10      351 non-null    float64\n",
      " 11  11      351 non-null    float64\n",
      " 12  12      351 non-null    float64\n",
      " 13  13      351 non-null    float64\n",
      " 14  14      351 non-null    float64\n",
      " 15  15      351 non-null    float64\n",
      " 16  16      351 non-null    float64\n",
      " 17  17      351 non-null    float64\n",
      " 18  18      351 non-null    float64\n",
      " 19  19      351 non-null    float64\n",
      " 20  20      351 non-null    float64\n",
      " 21  21      351 non-null    float64\n",
      " 22  22      351 non-null    float64\n",
      " 23  23      351 non-null    float64\n",
      " 24  24      351 non-null    float64\n",
      " 25  25      351 non-null    float64\n",
      " 26  26      351 non-null    float64\n",
      " 27  27      351 non-null    float64\n",
      " 28  28      351 non-null    float64\n",
      " 29  29      351 non-null    float64\n",
      " 30  30      351 non-null    float64\n",
      " 31  31      351 non-null    float64\n",
      " 32  32      351 non-null    float64\n",
      " 33  33      351 non-null    float64\n",
      " 34  34      351 non-null    object \n",
      "dtypes: float64(32), int64(2), object(1)\n",
      "memory usage: 96.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0.99539 ... -0.54487 0.18641 -0.453]\n",
      " [1 0 1.0 ... -0.06287999999999999 -0.13738 -0.02447]\n",
      " [1 0 1.0 ... -0.2418 0.56045 -0.38238]\n",
      " ...\n",
      " [1 0 0.94701 ... 0.00442 0.9269700000000001 -0.00577]\n",
      " [1 0 0.9060799999999999 ... -0.03757 0.87403 -0.16243]\n",
      " [1 0 0.8471 ... -0.06677999999999999 0.85764 -0.06151]]\n",
      "\n",
      "['g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n",
      " 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n",
      " 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n",
      " 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n",
      " 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b'\n",
      " 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n",
      " 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n",
      " 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n",
      " 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n",
      " 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n",
      " 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n",
      " 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n",
      " 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n",
      " 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g' 'b' 'g'\n",
      " 'b' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g']\n"
     ]
    }
   ],
   "source": [
    "# split into input and output columns\n",
    "X, y = df.values[:, :-1], df.values[:, -1]\n",
    "print(X)\n",
    "print()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making all data floating point values\n",
    "X = X.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode label: strings to integer\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235, 34) (116, 34) (235,) (116,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]  #taking all columns as features\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "#1st hidden layer with 10 nodes, taking 34 features as input\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "\n",
    "#2nd hidden layer with 8 nodes\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "\n",
    "#output layer with 1 node(neuron)\n",
    "model.add(Dense(1, activation='tanh'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "8/8 - 0s - loss: 2.6787 - accuracy: 0.7319\n",
      "Epoch 2/150\n",
      "8/8 - 0s - loss: 2.4679 - accuracy: 0.7447\n",
      "Epoch 3/150\n",
      "8/8 - 0s - loss: 2.3802 - accuracy: 0.7447\n",
      "Epoch 4/150\n",
      "8/8 - 0s - loss: 2.2621 - accuracy: 0.7574\n",
      "Epoch 5/150\n",
      "8/8 - 0s - loss: 2.0909 - accuracy: 0.7532\n",
      "Epoch 6/150\n",
      "8/8 - 0s - loss: 1.6903 - accuracy: 0.7617\n",
      "Epoch 7/150\n",
      "8/8 - 0s - loss: 1.2203 - accuracy: 0.7617\n",
      "Epoch 8/150\n",
      "8/8 - 0s - loss: 1.0089 - accuracy: 0.7660\n",
      "Epoch 9/150\n",
      "8/8 - 0s - loss: 0.9264 - accuracy: 0.7702\n",
      "Epoch 10/150\n",
      "8/8 - 0s - loss: 0.9027 - accuracy: 0.7745\n",
      "Epoch 11/150\n",
      "8/8 - 0s - loss: 0.8877 - accuracy: 0.7745\n",
      "Epoch 12/150\n",
      "8/8 - 0s - loss: 0.8741 - accuracy: 0.7872\n",
      "Epoch 13/150\n",
      "8/8 - 0s - loss: 0.8649 - accuracy: 0.7872\n",
      "Epoch 14/150\n",
      "8/8 - 0s - loss: 0.8549 - accuracy: 0.7915\n",
      "Epoch 15/150\n",
      "8/8 - 0s - loss: 0.8456 - accuracy: 0.7957\n",
      "Epoch 16/150\n",
      "8/8 - 0s - loss: 0.8353 - accuracy: 0.8043\n",
      "Epoch 17/150\n",
      "8/8 - 0s - loss: 0.8235 - accuracy: 0.8085\n",
      "Epoch 18/150\n",
      "8/8 - 0s - loss: 0.8124 - accuracy: 0.8128\n",
      "Epoch 19/150\n",
      "8/8 - 0s - loss: 0.8037 - accuracy: 0.8170\n",
      "Epoch 20/150\n",
      "8/8 - 0s - loss: 0.7956 - accuracy: 0.8255\n",
      "Epoch 21/150\n",
      "8/8 - 0s - loss: 0.7897 - accuracy: 0.8298\n",
      "Epoch 22/150\n",
      "8/8 - 0s - loss: 0.7815 - accuracy: 0.8298\n",
      "Epoch 23/150\n",
      "8/8 - 0s - loss: 0.7738 - accuracy: 0.8426\n",
      "Epoch 24/150\n",
      "8/8 - 0s - loss: 0.7663 - accuracy: 0.8596\n",
      "Epoch 25/150\n",
      "8/8 - 0s - loss: 0.7592 - accuracy: 0.8766\n",
      "Epoch 26/150\n",
      "8/8 - 0s - loss: 0.7521 - accuracy: 0.8851\n",
      "Epoch 27/150\n",
      "8/8 - 0s - loss: 0.7455 - accuracy: 0.8979\n",
      "Epoch 28/150\n",
      "8/8 - 0s - loss: 0.7388 - accuracy: 0.8979\n",
      "Epoch 29/150\n",
      "8/8 - 0s - loss: 0.7335 - accuracy: 0.8979\n",
      "Epoch 30/150\n",
      "8/8 - 0s - loss: 0.7275 - accuracy: 0.8979\n",
      "Epoch 31/150\n",
      "8/8 - 0s - loss: 0.7225 - accuracy: 0.9021\n",
      "Epoch 32/150\n",
      "8/8 - 0s - loss: 0.7175 - accuracy: 0.9021\n",
      "Epoch 33/150\n",
      "8/8 - 0s - loss: 0.7120 - accuracy: 0.9021\n",
      "Epoch 34/150\n",
      "8/8 - 0s - loss: 0.7074 - accuracy: 0.9021\n",
      "Epoch 35/150\n",
      "8/8 - 0s - loss: 0.7030 - accuracy: 0.8979\n",
      "Epoch 36/150\n",
      "8/8 - 0s - loss: 0.6993 - accuracy: 0.9064\n",
      "Epoch 37/150\n",
      "8/8 - 0s - loss: 0.6947 - accuracy: 0.9149\n",
      "Epoch 38/150\n",
      "8/8 - 0s - loss: 0.6908 - accuracy: 0.9191\n",
      "Epoch 39/150\n",
      "8/8 - 0s - loss: 0.6870 - accuracy: 0.9191\n",
      "Epoch 40/150\n",
      "8/8 - 0s - loss: 0.6837 - accuracy: 0.9277\n",
      "Epoch 41/150\n",
      "8/8 - 0s - loss: 0.6800 - accuracy: 0.9234\n",
      "Epoch 42/150\n",
      "8/8 - 0s - loss: 0.6766 - accuracy: 0.9234\n",
      "Epoch 43/150\n",
      "8/8 - 0s - loss: 0.6740 - accuracy: 0.9234\n",
      "Epoch 44/150\n",
      "8/8 - 0s - loss: 0.6712 - accuracy: 0.9191\n",
      "Epoch 45/150\n",
      "8/8 - 0s - loss: 0.6669 - accuracy: 0.9277\n",
      "Epoch 46/150\n",
      "8/8 - 0s - loss: 0.6638 - accuracy: 0.9277\n",
      "Epoch 47/150\n",
      "8/8 - 0s - loss: 0.6609 - accuracy: 0.9277\n",
      "Epoch 48/150\n",
      "8/8 - 0s - loss: 0.6578 - accuracy: 0.9277\n",
      "Epoch 49/150\n",
      "8/8 - 0s - loss: 0.6549 - accuracy: 0.9319\n",
      "Epoch 50/150\n",
      "8/8 - 0s - loss: 0.6522 - accuracy: 0.9319\n",
      "Epoch 51/150\n",
      "8/8 - 0s - loss: 0.6497 - accuracy: 0.9277\n",
      "Epoch 52/150\n",
      "8/8 - 0s - loss: 0.6470 - accuracy: 0.9319\n",
      "Epoch 53/150\n",
      "8/8 - 0s - loss: 0.6443 - accuracy: 0.9319\n",
      "Epoch 54/150\n",
      "8/8 - 0s - loss: 0.6424 - accuracy: 0.9362\n",
      "Epoch 55/150\n",
      "8/8 - 0s - loss: 0.6398 - accuracy: 0.9362\n",
      "Epoch 56/150\n",
      "8/8 - 0s - loss: 0.6369 - accuracy: 0.9404\n",
      "Epoch 57/150\n",
      "8/8 - 0s - loss: 0.6357 - accuracy: 0.9404\n",
      "Epoch 58/150\n",
      "8/8 - 0s - loss: 0.6332 - accuracy: 0.9404\n",
      "Epoch 59/150\n",
      "8/8 - 0s - loss: 0.6314 - accuracy: 0.9404\n",
      "Epoch 60/150\n",
      "8/8 - 0s - loss: 0.6296 - accuracy: 0.9404\n",
      "Epoch 61/150\n",
      "8/8 - 0s - loss: 0.6280 - accuracy: 0.9447\n",
      "Epoch 62/150\n",
      "8/8 - 0s - loss: 0.6269 - accuracy: 0.9447\n",
      "Epoch 63/150\n",
      "8/8 - 0s - loss: 0.6259 - accuracy: 0.9447\n",
      "Epoch 64/150\n",
      "8/8 - 0s - loss: 0.6240 - accuracy: 0.9447\n",
      "Epoch 65/150\n",
      "8/8 - 0s - loss: 0.6227 - accuracy: 0.9447\n",
      "Epoch 66/150\n",
      "8/8 - 0s - loss: 0.6206 - accuracy: 0.9447\n",
      "Epoch 67/150\n",
      "8/8 - 0s - loss: 0.6202 - accuracy: 0.9447\n",
      "Epoch 68/150\n",
      "8/8 - 0s - loss: 0.6187 - accuracy: 0.9447\n",
      "Epoch 69/150\n",
      "8/8 - 0s - loss: 0.6172 - accuracy: 0.9489\n",
      "Epoch 70/150\n",
      "8/8 - 0s - loss: 0.6160 - accuracy: 0.9489\n",
      "Epoch 71/150\n",
      "8/8 - 0s - loss: 0.6143 - accuracy: 0.9489\n",
      "Epoch 72/150\n",
      "8/8 - 0s - loss: 0.6128 - accuracy: 0.9489\n",
      "Epoch 73/150\n",
      "8/8 - 0s - loss: 0.6115 - accuracy: 0.9489\n",
      "Epoch 74/150\n",
      "8/8 - 0s - loss: 0.6106 - accuracy: 0.9489\n",
      "Epoch 75/150\n",
      "8/8 - 0s - loss: 0.6093 - accuracy: 0.9489\n",
      "Epoch 76/150\n",
      "8/8 - 0s - loss: 0.6085 - accuracy: 0.9489\n",
      "Epoch 77/150\n",
      "8/8 - 0s - loss: 0.6073 - accuracy: 0.9489\n",
      "Epoch 78/150\n",
      "8/8 - 0s - loss: 0.6063 - accuracy: 0.9489\n",
      "Epoch 79/150\n",
      "8/8 - 0s - loss: 0.6051 - accuracy: 0.9489\n",
      "Epoch 80/150\n",
      "8/8 - 0s - loss: 0.6042 - accuracy: 0.9532\n",
      "Epoch 81/150\n",
      "8/8 - 0s - loss: 0.6032 - accuracy: 0.9532\n",
      "Epoch 82/150\n",
      "8/8 - 0s - loss: 0.6026 - accuracy: 0.9532\n",
      "Epoch 83/150\n",
      "8/8 - 0s - loss: 0.6012 - accuracy: 0.9532\n",
      "Epoch 84/150\n",
      "8/8 - 0s - loss: 0.6009 - accuracy: 0.9532\n",
      "Epoch 85/150\n",
      "8/8 - 0s - loss: 0.5997 - accuracy: 0.9532\n",
      "Epoch 86/150\n",
      "8/8 - 0s - loss: 0.5984 - accuracy: 0.9532\n",
      "Epoch 87/150\n",
      "8/8 - 0s - loss: 0.5983 - accuracy: 0.9532\n",
      "Epoch 88/150\n",
      "8/8 - 0s - loss: 0.5962 - accuracy: 0.9532\n",
      "Epoch 89/150\n",
      "8/8 - 0s - loss: 0.5954 - accuracy: 0.9532\n",
      "Epoch 90/150\n",
      "8/8 - 0s - loss: 0.5941 - accuracy: 0.9532\n",
      "Epoch 91/150\n",
      "8/8 - 0s - loss: 0.5933 - accuracy: 0.9532\n",
      "Epoch 92/150\n",
      "8/8 - 0s - loss: 0.5923 - accuracy: 0.9532\n",
      "Epoch 93/150\n",
      "8/8 - 0s - loss: 0.5916 - accuracy: 0.9532\n",
      "Epoch 94/150\n",
      "8/8 - 0s - loss: 0.5911 - accuracy: 0.9532\n",
      "Epoch 95/150\n",
      "8/8 - 0s - loss: 0.5898 - accuracy: 0.9532\n",
      "Epoch 96/150\n",
      "8/8 - 0s - loss: 0.5891 - accuracy: 0.9532\n",
      "Epoch 97/150\n",
      "8/8 - 0s - loss: 0.5882 - accuracy: 0.9532\n",
      "Epoch 98/150\n",
      "8/8 - 0s - loss: 0.5877 - accuracy: 0.9532\n",
      "Epoch 99/150\n",
      "8/8 - 0s - loss: 0.5867 - accuracy: 0.9532\n",
      "Epoch 100/150\n",
      "8/8 - 0s - loss: 0.5861 - accuracy: 0.9532\n",
      "Epoch 101/150\n",
      "8/8 - 0s - loss: 0.5854 - accuracy: 0.9532\n",
      "Epoch 102/150\n",
      "8/8 - 0s - loss: 0.5848 - accuracy: 0.9532\n",
      "Epoch 103/150\n",
      "8/8 - 0s - loss: 0.5846 - accuracy: 0.9532\n",
      "Epoch 104/150\n",
      "8/8 - 0s - loss: 0.5837 - accuracy: 0.9532\n",
      "Epoch 105/150\n",
      "8/8 - 0s - loss: 0.5831 - accuracy: 0.9532\n",
      "Epoch 106/150\n",
      "8/8 - 0s - loss: 0.5822 - accuracy: 0.9532\n",
      "Epoch 107/150\n",
      "8/8 - 0s - loss: 0.5831 - accuracy: 0.9532\n",
      "Epoch 108/150\n",
      "8/8 - 0s - loss: 0.5808 - accuracy: 0.9532\n",
      "Epoch 109/150\n",
      "8/8 - 0s - loss: 0.5806 - accuracy: 0.9532\n",
      "Epoch 110/150\n",
      "8/8 - 0s - loss: 0.5800 - accuracy: 0.9532\n",
      "Epoch 111/150\n",
      "8/8 - 0s - loss: 0.5809 - accuracy: 0.9532\n",
      "Epoch 112/150\n",
      "8/8 - 0s - loss: 0.5794 - accuracy: 0.9532\n",
      "Epoch 113/150\n",
      "8/8 - 0s - loss: 0.5783 - accuracy: 0.9532\n",
      "Epoch 114/150\n",
      "8/8 - 0s - loss: 0.5781 - accuracy: 0.9532\n",
      "Epoch 115/150\n",
      "8/8 - 0s - loss: 0.5771 - accuracy: 0.9532\n",
      "Epoch 116/150\n",
      "8/8 - 0s - loss: 0.5769 - accuracy: 0.9532\n",
      "Epoch 117/150\n",
      "8/8 - 0s - loss: 0.5768 - accuracy: 0.9532\n",
      "Epoch 118/150\n",
      "8/8 - 0s - loss: 0.5762 - accuracy: 0.9532\n",
      "Epoch 119/150\n",
      "8/8 - 0s - loss: 0.5759 - accuracy: 0.9532\n",
      "Epoch 120/150\n",
      "8/8 - 0s - loss: 0.5747 - accuracy: 0.9532\n",
      "Epoch 121/150\n",
      "8/8 - 0s - loss: 0.5748 - accuracy: 0.9532\n",
      "Epoch 122/150\n",
      "8/8 - 0s - loss: 0.5744 - accuracy: 0.9532\n",
      "Epoch 123/150\n",
      "8/8 - 0s - loss: 0.5742 - accuracy: 0.9532\n",
      "Epoch 124/150\n",
      "8/8 - 0s - loss: 0.5741 - accuracy: 0.9532\n",
      "Epoch 125/150\n",
      "8/8 - 0s - loss: 0.5736 - accuracy: 0.9532\n",
      "Epoch 126/150\n",
      "8/8 - 0s - loss: 0.5730 - accuracy: 0.9532\n",
      "Epoch 127/150\n",
      "8/8 - 0s - loss: 0.5727 - accuracy: 0.9532\n",
      "Epoch 128/150\n",
      "8/8 - 0s - loss: 0.5726 - accuracy: 0.9532\n",
      "Epoch 129/150\n",
      "8/8 - 0s - loss: 0.5723 - accuracy: 0.9532\n",
      "Epoch 130/150\n",
      "8/8 - 0s - loss: 0.5719 - accuracy: 0.9532\n",
      "Epoch 131/150\n",
      "8/8 - 0s - loss: 0.5720 - accuracy: 0.9574\n",
      "Epoch 132/150\n",
      "8/8 - 0s - loss: 0.5720 - accuracy: 0.9574\n",
      "Epoch 133/150\n",
      "8/8 - 0s - loss: 0.5713 - accuracy: 0.9574\n",
      "Epoch 134/150\n",
      "8/8 - 0s - loss: 0.5712 - accuracy: 0.9574\n",
      "Epoch 135/150\n",
      "8/8 - 0s - loss: 0.5709 - accuracy: 0.9574\n",
      "Epoch 136/150\n",
      "8/8 - 0s - loss: 0.5703 - accuracy: 0.9574\n",
      "Epoch 137/150\n",
      "8/8 - 0s - loss: 0.5707 - accuracy: 0.9574\n",
      "Epoch 138/150\n",
      "8/8 - 0s - loss: 0.5704 - accuracy: 0.9574\n",
      "Epoch 139/150\n",
      "8/8 - 0s - loss: 0.5699 - accuracy: 0.9574\n",
      "Epoch 140/150\n",
      "8/8 - 0s - loss: 0.5694 - accuracy: 0.9574\n",
      "Epoch 141/150\n",
      "8/8 - 0s - loss: 0.5695 - accuracy: 0.9574\n",
      "Epoch 142/150\n",
      "8/8 - 0s - loss: 0.5688 - accuracy: 0.9574\n",
      "Epoch 143/150\n",
      "8/8 - 0s - loss: 0.5693 - accuracy: 0.9574\n",
      "Epoch 144/150\n",
      "8/8 - 0s - loss: 0.5689 - accuracy: 0.9574\n",
      "Epoch 145/150\n",
      "8/8 - 0s - loss: 0.5684 - accuracy: 0.9574\n",
      "Epoch 146/150\n",
      "8/8 - 0s - loss: 0.5681 - accuracy: 0.9574\n",
      "Epoch 147/150\n",
      "8/8 - 0s - loss: 0.5678 - accuracy: 0.9574\n",
      "Epoch 148/150\n",
      "8/8 - 0s - loss: 0.5676 - accuracy: 0.9574\n",
      "Epoch 149/150\n",
      "8/8 - 0s - loss: 0.5675 - accuracy: 0.9574\n",
      "Epoch 150/150\n",
      "8/8 - 0s - loss: 0.5673 - accuracy: 0.9574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12b5ba47b80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.914\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 1.0\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "y_p = model.predict([row])\n",
    "print('Predicted: %.1f' % y_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP for Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "from numpy import argmax\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3               4\n",
       "0    5.1  3.5  1.4  0.2     Iris-setosa\n",
       "1    4.9  3.0  1.4  0.2     Iris-setosa\n",
       "2    4.7  3.2  1.3  0.2     Iris-setosa\n",
       "3    4.6  3.1  1.5  0.2     Iris-setosa\n",
       "4    5.0  3.6  1.4  0.2     Iris-setosa\n",
       "..   ...  ...  ...  ...             ...\n",
       "145  6.7  3.0  5.2  2.3  Iris-virginica\n",
       "146  6.3  2.5  5.0  1.9  Iris-virginica\n",
       "147  6.5  3.0  5.2  2.0  Iris-virginica\n",
       "148  6.2  3.4  5.4  2.3  Iris-virginica\n",
       "149  5.9  3.0  5.1  1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset : Iris Dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
    "df = read_csv(path, header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.0 1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.0 3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.0 3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.0 1.4 0.1]\n",
      " [4.3 3.0 1.1 0.1]\n",
      " [5.8 4.0 1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.0 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.0 3.0 1.6 0.2]\n",
      " [5.0 3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.0 3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.0 1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.0 3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.0 3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.0 1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.0 3.3 1.4 0.2]\n",
      " [7.0 3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.0 1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1.0]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.0 2.0 3.5 1.0]\n",
      " [5.9 3.0 4.2 1.5]\n",
      " [6.0 2.2 4.0 1.0]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.0 4.5 1.5]\n",
      " [5.8 2.7 4.1 1.0]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.0 1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.0 4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.0 5.0 1.7]\n",
      " [6.0 2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1.0]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1.0]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.0 2.7 5.1 1.6]\n",
      " [5.4 3.0 4.5 1.5]\n",
      " [6.0 3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.0 4.1 1.3]\n",
      " [5.5 2.5 4.0 1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.0 4.6 1.4]\n",
      " [5.8 2.6 4.0 1.2]\n",
      " [5.0 2.3 3.3 1.0]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.0 4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.0 1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.0 2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.0 5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.0 5.8 2.2]\n",
      " [7.6 3.0 6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2.0]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.0 5.5 2.1]\n",
      " [5.7 2.5 5.0 2.0]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.0 5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.0 2.2 5.0 1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2.0]\n",
      " [7.7 2.8 6.7 2.0]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.0 1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.0 4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.0 5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2.0]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.0 6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.0 3.0 4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.0 5.2 2.3]\n",
      " [6.3 2.5 5.0 1.9]\n",
      " [6.5 3.0 5.2 2.0]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.0 5.1 1.8]]\n",
      "\n",
      "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "# split into input and output columns\n",
    "X, y = df.values[:, :-1], df.values[:, -1]\n",
    "print(X)\n",
    "print()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure all data are floating point values\n",
    "X = X.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode strings to integer\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4) (50, 4) (100,) (50,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "#1st hidden layer, taking 4 features as input\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "\n",
    "#2nd hidden layer\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "\n",
    "#output layer with 3 nodes\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "4/4 - 0s - loss: 3.0341 - accuracy: 0.3500\n",
      "Epoch 2/150\n",
      "4/4 - 0s - loss: 2.8281 - accuracy: 0.5200\n",
      "Epoch 3/150\n",
      "4/4 - 0s - loss: 2.6318 - accuracy: 0.6200\n",
      "Epoch 4/150\n",
      "4/4 - 0s - loss: 2.4471 - accuracy: 0.6400\n",
      "Epoch 5/150\n",
      "4/4 - 0s - loss: 2.2476 - accuracy: 0.6400\n",
      "Epoch 6/150\n",
      "4/4 - 0s - loss: 2.0571 - accuracy: 0.6400\n",
      "Epoch 7/150\n",
      "4/4 - 0s - loss: 1.8755 - accuracy: 0.6400\n",
      "Epoch 8/150\n",
      "4/4 - 0s - loss: 1.6888 - accuracy: 0.6400\n",
      "Epoch 9/150\n",
      "4/4 - 0s - loss: 1.5203 - accuracy: 0.6400\n",
      "Epoch 10/150\n",
      "4/4 - 0s - loss: 1.3415 - accuracy: 0.6400\n",
      "Epoch 11/150\n",
      "4/4 - 0s - loss: 1.1742 - accuracy: 0.6400\n",
      "Epoch 12/150\n",
      "4/4 - 0s - loss: 1.0442 - accuracy: 0.6400\n",
      "Epoch 13/150\n",
      "4/4 - 0s - loss: 0.9152 - accuracy: 0.6400\n",
      "Epoch 14/150\n",
      "4/4 - 0s - loss: 0.7997 - accuracy: 0.6400\n",
      "Epoch 15/150\n",
      "4/4 - 0s - loss: 0.7067 - accuracy: 0.6400\n",
      "Epoch 16/150\n",
      "4/4 - 0s - loss: 0.6268 - accuracy: 0.6500\n",
      "Epoch 17/150\n",
      "4/4 - 0s - loss: 0.5620 - accuracy: 0.7600\n",
      "Epoch 18/150\n",
      "4/4 - 0s - loss: 0.5163 - accuracy: 0.8400\n",
      "Epoch 19/150\n",
      "4/4 - 0s - loss: 0.4792 - accuracy: 0.9300\n",
      "Epoch 20/150\n",
      "4/4 - 0s - loss: 0.4510 - accuracy: 0.9700\n",
      "Epoch 21/150\n",
      "4/4 - 0s - loss: 0.4283 - accuracy: 0.9700\n",
      "Epoch 22/150\n",
      "4/4 - 0s - loss: 0.4114 - accuracy: 0.9700\n",
      "Epoch 23/150\n",
      "4/4 - 0s - loss: 0.3975 - accuracy: 0.9700\n",
      "Epoch 24/150\n",
      "4/4 - 0s - loss: 0.3841 - accuracy: 0.9800\n",
      "Epoch 25/150\n",
      "4/4 - 0s - loss: 0.3720 - accuracy: 0.9800\n",
      "Epoch 26/150\n",
      "4/4 - 0s - loss: 0.3615 - accuracy: 0.9700\n",
      "Epoch 27/150\n",
      "4/4 - 0s - loss: 0.3521 - accuracy: 0.9700\n",
      "Epoch 28/150\n",
      "4/4 - 0s - loss: 0.3431 - accuracy: 0.9700\n",
      "Epoch 29/150\n",
      "4/4 - 0s - loss: 0.3349 - accuracy: 0.9800\n",
      "Epoch 30/150\n",
      "4/4 - 0s - loss: 0.3273 - accuracy: 0.9800\n",
      "Epoch 31/150\n",
      "4/4 - 0s - loss: 0.3196 - accuracy: 0.9800\n",
      "Epoch 32/150\n",
      "4/4 - 0s - loss: 0.3126 - accuracy: 0.9800\n",
      "Epoch 33/150\n",
      "4/4 - 0s - loss: 0.3063 - accuracy: 0.9800\n",
      "Epoch 34/150\n",
      "4/4 - 0s - loss: 0.3000 - accuracy: 0.9800\n",
      "Epoch 35/150\n",
      "4/4 - 0s - loss: 0.2936 - accuracy: 0.9800\n",
      "Epoch 36/150\n",
      "4/4 - 0s - loss: 0.2883 - accuracy: 0.9800\n",
      "Epoch 37/150\n",
      "4/4 - 0s - loss: 0.2837 - accuracy: 0.9800\n",
      "Epoch 38/150\n",
      "4/4 - 0s - loss: 0.2790 - accuracy: 0.9800\n",
      "Epoch 39/150\n",
      "4/4 - 0s - loss: 0.2743 - accuracy: 0.9800\n",
      "Epoch 40/150\n",
      "4/4 - 0s - loss: 0.2694 - accuracy: 0.9800\n",
      "Epoch 41/150\n",
      "4/4 - 0s - loss: 0.2646 - accuracy: 0.9800\n",
      "Epoch 42/150\n",
      "4/4 - 0s - loss: 0.2594 - accuracy: 0.9800\n",
      "Epoch 43/150\n",
      "4/4 - 0s - loss: 0.2547 - accuracy: 0.9800\n",
      "Epoch 44/150\n",
      "4/4 - 0s - loss: 0.2518 - accuracy: 0.9700\n",
      "Epoch 45/150\n",
      "4/4 - 0s - loss: 0.2483 - accuracy: 0.9600\n",
      "Epoch 46/150\n",
      "4/4 - 0s - loss: 0.2438 - accuracy: 0.9600\n",
      "Epoch 47/150\n",
      "4/4 - 0s - loss: 0.2401 - accuracy: 0.9800\n",
      "Epoch 48/150\n",
      "4/4 - 0s - loss: 0.2363 - accuracy: 0.9800\n",
      "Epoch 49/150\n",
      "4/4 - 0s - loss: 0.2332 - accuracy: 0.9800\n",
      "Epoch 50/150\n",
      "4/4 - 0s - loss: 0.2299 - accuracy: 0.9800\n",
      "Epoch 51/150\n",
      "4/4 - 0s - loss: 0.2263 - accuracy: 0.9800\n",
      "Epoch 52/150\n",
      "4/4 - 0s - loss: 0.2226 - accuracy: 0.9800\n",
      "Epoch 53/150\n",
      "4/4 - 0s - loss: 0.2200 - accuracy: 0.9800\n",
      "Epoch 54/150\n",
      "4/4 - 0s - loss: 0.2174 - accuracy: 0.9800\n",
      "Epoch 55/150\n",
      "4/4 - 0s - loss: 0.2145 - accuracy: 0.9800\n",
      "Epoch 56/150\n",
      "4/4 - 0s - loss: 0.2119 - accuracy: 0.9800\n",
      "Epoch 57/150\n",
      "4/4 - 0s - loss: 0.2111 - accuracy: 0.9600\n",
      "Epoch 58/150\n",
      "4/4 - 0s - loss: 0.2086 - accuracy: 0.9600\n",
      "Epoch 59/150\n",
      "4/4 - 0s - loss: 0.2060 - accuracy: 0.9600\n",
      "Epoch 60/150\n",
      "4/4 - 0s - loss: 0.2037 - accuracy: 0.9600\n",
      "Epoch 61/150\n",
      "4/4 - 0s - loss: 0.2019 - accuracy: 0.9600\n",
      "Epoch 62/150\n",
      "4/4 - 0s - loss: 0.2004 - accuracy: 0.9600\n",
      "Epoch 63/150\n",
      "4/4 - 0s - loss: 0.1975 - accuracy: 0.9700\n",
      "Epoch 64/150\n",
      "4/4 - 0s - loss: 0.1948 - accuracy: 0.9800\n",
      "Epoch 65/150\n",
      "4/4 - 0s - loss: 0.1932 - accuracy: 0.9800\n",
      "Epoch 66/150\n",
      "4/4 - 0s - loss: 0.1914 - accuracy: 0.9800\n",
      "Epoch 67/150\n",
      "4/4 - 0s - loss: 0.1894 - accuracy: 0.9800\n",
      "Epoch 68/150\n",
      "4/4 - 0s - loss: 0.1877 - accuracy: 0.9800\n",
      "Epoch 69/150\n",
      "4/4 - 0s - loss: 0.1858 - accuracy: 0.9800\n",
      "Epoch 70/150\n",
      "4/4 - 0s - loss: 0.1846 - accuracy: 0.9800\n",
      "Epoch 71/150\n",
      "4/4 - 0s - loss: 0.1828 - accuracy: 0.9800\n",
      "Epoch 72/150\n",
      "4/4 - 0s - loss: 0.1813 - accuracy: 0.9600\n",
      "Epoch 73/150\n",
      "4/4 - 0s - loss: 0.1812 - accuracy: 0.9600\n",
      "Epoch 74/150\n",
      "4/4 - 0s - loss: 0.1809 - accuracy: 0.9500\n",
      "Epoch 75/150\n",
      "4/4 - 0s - loss: 0.1809 - accuracy: 0.9500\n",
      "Epoch 76/150\n",
      "4/4 - 0s - loss: 0.1805 - accuracy: 0.9500\n",
      "Epoch 77/150\n",
      "4/4 - 0s - loss: 0.1759 - accuracy: 0.9600\n",
      "Epoch 78/150\n",
      "4/4 - 0s - loss: 0.1737 - accuracy: 0.9700\n",
      "Epoch 79/150\n",
      "4/4 - 0s - loss: 0.1715 - accuracy: 0.9800\n",
      "Epoch 80/150\n",
      "4/4 - 0s - loss: 0.1710 - accuracy: 0.9700\n",
      "Epoch 81/150\n",
      "4/4 - 0s - loss: 0.1701 - accuracy: 0.9600\n",
      "Epoch 82/150\n",
      "4/4 - 0s - loss: 0.1689 - accuracy: 0.9600\n",
      "Epoch 83/150\n",
      "4/4 - 0s - loss: 0.1678 - accuracy: 0.9800\n",
      "Epoch 84/150\n",
      "4/4 - 0s - loss: 0.1661 - accuracy: 0.9800\n",
      "Epoch 85/150\n",
      "4/4 - 0s - loss: 0.1650 - accuracy: 0.9800\n",
      "Epoch 86/150\n",
      "4/4 - 0s - loss: 0.1639 - accuracy: 0.9800\n",
      "Epoch 87/150\n",
      "4/4 - 0s - loss: 0.1650 - accuracy: 0.9800\n",
      "Epoch 88/150\n",
      "4/4 - 0s - loss: 0.1656 - accuracy: 0.9800\n",
      "Epoch 89/150\n",
      "4/4 - 0s - loss: 0.1643 - accuracy: 0.9800\n",
      "Epoch 90/150\n",
      "4/4 - 0s - loss: 0.1613 - accuracy: 0.9800\n",
      "Epoch 91/150\n",
      "4/4 - 0s - loss: 0.1593 - accuracy: 0.9800\n",
      "Epoch 92/150\n",
      "4/4 - 0s - loss: 0.1588 - accuracy: 0.9600\n",
      "Epoch 93/150\n",
      "4/4 - 0s - loss: 0.1599 - accuracy: 0.9500\n",
      "Epoch 94/150\n",
      "4/4 - 0s - loss: 0.1615 - accuracy: 0.9500\n",
      "Epoch 95/150\n",
      "4/4 - 0s - loss: 0.1598 - accuracy: 0.9500\n",
      "Epoch 96/150\n",
      "4/4 - 0s - loss: 0.1564 - accuracy: 0.9600\n",
      "Epoch 97/150\n",
      "4/4 - 0s - loss: 0.1530 - accuracy: 0.9800\n",
      "Epoch 98/150\n",
      "4/4 - 0s - loss: 0.1535 - accuracy: 0.9800\n",
      "Epoch 99/150\n",
      "4/4 - 0s - loss: 0.1533 - accuracy: 0.9800\n",
      "Epoch 100/150\n",
      "4/4 - 0s - loss: 0.1521 - accuracy: 0.9800\n",
      "Epoch 101/150\n",
      "4/4 - 0s - loss: 0.1510 - accuracy: 0.9800\n",
      "Epoch 102/150\n",
      "4/4 - 0s - loss: 0.1502 - accuracy: 0.9800\n",
      "Epoch 103/150\n",
      "4/4 - 0s - loss: 0.1489 - accuracy: 0.9800\n",
      "Epoch 104/150\n",
      "4/4 - 0s - loss: 0.1471 - accuracy: 0.9800\n",
      "Epoch 105/150\n",
      "4/4 - 0s - loss: 0.1466 - accuracy: 0.9800\n",
      "Epoch 106/150\n",
      "4/4 - 0s - loss: 0.1458 - accuracy: 0.9800\n",
      "Epoch 107/150\n",
      "4/4 - 0s - loss: 0.1452 - accuracy: 0.9800\n",
      "Epoch 108/150\n",
      "4/4 - 0s - loss: 0.1444 - accuracy: 0.9800\n",
      "Epoch 109/150\n",
      "4/4 - 0s - loss: 0.1433 - accuracy: 0.9800\n",
      "Epoch 110/150\n",
      "4/4 - 0s - loss: 0.1422 - accuracy: 0.9800\n",
      "Epoch 111/150\n",
      "4/4 - 0s - loss: 0.1426 - accuracy: 0.9800\n",
      "Epoch 112/150\n",
      "4/4 - 0s - loss: 0.1428 - accuracy: 0.9800\n",
      "Epoch 113/150\n",
      "4/4 - 0s - loss: 0.1404 - accuracy: 0.9800\n",
      "Epoch 114/150\n",
      "4/4 - 0s - loss: 0.1397 - accuracy: 0.9800\n",
      "Epoch 115/150\n",
      "4/4 - 0s - loss: 0.1393 - accuracy: 0.9800\n",
      "Epoch 116/150\n",
      "4/4 - 0s - loss: 0.1387 - accuracy: 0.9700\n",
      "Epoch 117/150\n",
      "4/4 - 0s - loss: 0.1381 - accuracy: 0.9700\n",
      "Epoch 118/150\n",
      "4/4 - 0s - loss: 0.1373 - accuracy: 0.9800\n",
      "Epoch 119/150\n",
      "4/4 - 0s - loss: 0.1363 - accuracy: 0.9800\n",
      "Epoch 120/150\n",
      "4/4 - 0s - loss: 0.1358 - accuracy: 0.9800\n",
      "Epoch 121/150\n",
      "4/4 - 0s - loss: 0.1351 - accuracy: 0.9800\n",
      "Epoch 122/150\n",
      "4/4 - 0s - loss: 0.1346 - accuracy: 0.9800\n",
      "Epoch 123/150\n",
      "4/4 - 0s - loss: 0.1338 - accuracy: 0.9800\n",
      "Epoch 124/150\n",
      "4/4 - 0s - loss: 0.1333 - accuracy: 0.9800\n",
      "Epoch 125/150\n",
      "4/4 - 0s - loss: 0.1330 - accuracy: 0.9800\n",
      "Epoch 126/150\n",
      "4/4 - 0s - loss: 0.1323 - accuracy: 0.9800\n",
      "Epoch 127/150\n",
      "4/4 - 0s - loss: 0.1316 - accuracy: 0.9800\n",
      "Epoch 128/150\n",
      "4/4 - 0s - loss: 0.1309 - accuracy: 0.9800\n",
      "Epoch 129/150\n",
      "4/4 - 0s - loss: 0.1304 - accuracy: 0.9800\n",
      "Epoch 130/150\n",
      "4/4 - 0s - loss: 0.1301 - accuracy: 0.9800\n",
      "Epoch 131/150\n",
      "4/4 - 0s - loss: 0.1304 - accuracy: 0.9800\n",
      "Epoch 132/150\n",
      "4/4 - 0s - loss: 0.1308 - accuracy: 0.9800\n",
      "Epoch 133/150\n",
      "4/4 - 0s - loss: 0.1305 - accuracy: 0.9800\n",
      "Epoch 134/150\n",
      "4/4 - 0s - loss: 0.1283 - accuracy: 0.9800\n",
      "Epoch 135/150\n",
      "4/4 - 0s - loss: 0.1302 - accuracy: 0.9600\n",
      "Epoch 136/150\n",
      "4/4 - 0s - loss: 0.1304 - accuracy: 0.9500\n",
      "Epoch 137/150\n",
      "4/4 - 0s - loss: 0.1295 - accuracy: 0.9600\n",
      "Epoch 138/150\n",
      "4/4 - 0s - loss: 0.1284 - accuracy: 0.9600\n",
      "Epoch 139/150\n",
      "4/4 - 0s - loss: 0.1267 - accuracy: 0.9600\n",
      "Epoch 140/150\n",
      "4/4 - 0s - loss: 0.1254 - accuracy: 0.9800\n",
      "Epoch 141/150\n",
      "4/4 - 0s - loss: 0.1245 - accuracy: 0.9800\n",
      "Epoch 142/150\n",
      "4/4 - 0s - loss: 0.1244 - accuracy: 0.9800\n",
      "Epoch 143/150\n",
      "4/4 - 0s - loss: 0.1235 - accuracy: 0.9800\n",
      "Epoch 144/150\n",
      "4/4 - 0s - loss: 0.1274 - accuracy: 0.9600\n",
      "Epoch 145/150\n",
      "4/4 - 0s - loss: 0.1282 - accuracy: 0.9500\n",
      "Epoch 146/150\n",
      "4/4 - 0s - loss: 0.1252 - accuracy: 0.9500\n",
      "Epoch 147/150\n",
      "4/4 - 0s - loss: 0.1231 - accuracy: 0.9800\n",
      "Epoch 148/150\n",
      "4/4 - 0s - loss: 0.1225 - accuracy: 0.9800\n",
      "Epoch 149/150\n",
      "4/4 - 0s - loss: 0.1226 - accuracy: 0.9800\n",
      "Epoch 150/150\n",
      "4/4 - 0s - loss: 0.1219 - accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12b5a4db430>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.960\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[9.9464607e-01 5.3539816e-03 9.8921031e-09]] (class=0)\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "row = [5.1,3.5,1.4,0.2]\n",
    "yhat = model.predict([row])\n",
    "print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp for regression\n",
    "from numpy import sqrt\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1      2   3      4      5     6       7   8      9     10  \\\n",
       "0    0.00632  18.0   2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
       "1    0.02731   0.0   7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
       "2    0.02729   0.0   7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
       "3    0.03237   0.0   2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
       "4    0.06905   0.0   2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
       "..       ...   ...    ...  ..    ...    ...   ...     ...  ..    ...   ...   \n",
       "501  0.06263   0.0  11.93   0  0.573  6.593  69.1  2.4786   1  273.0  21.0   \n",
       "502  0.04527   0.0  11.93   0  0.573  6.120  76.7  2.2875   1  273.0  21.0   \n",
       "503  0.06076   0.0  11.93   0  0.573  6.976  91.0  2.1675   1  273.0  21.0   \n",
       "504  0.10959   0.0  11.93   0  0.573  6.794  89.3  2.3889   1  273.0  21.0   \n",
       "505  0.04741   0.0  11.93   0  0.573  6.030  80.8  2.5050   1  273.0  21.0   \n",
       "\n",
       "         11    12    13  \n",
       "0    396.90  4.98  24.0  \n",
       "1    396.90  9.14  21.6  \n",
       "2    392.83  4.03  34.7  \n",
       "3    394.63  2.94  33.4  \n",
       "4    396.90  5.33  36.2  \n",
       "..      ...   ...   ...  \n",
       "501  391.99  9.67  22.4  \n",
       "502  396.90  9.08  20.6  \n",
       "503  396.90  5.64  23.9  \n",
       "504  393.45  6.48  22.0  \n",
       "505  396.90  7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "df = read_csv(path, header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n",
      "\n",
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n"
     ]
    }
   ],
   "source": [
    "# split into input and output columns\n",
    "X, y = df.values[:, :-1], df.values[:, -1]\n",
    "print(X)\n",
    "print()\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339, 13) (167, 13) (339,) (167,)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# determine the number of input features\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', kernel_initializer='he_normal', input_shape=(n_features,)))\n",
    "model.add(Dense(8, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "11/11 - 0s - loss: 52.8083\n",
      "Epoch 2/150\n",
      "11/11 - 0s - loss: 53.8773\n",
      "Epoch 3/150\n",
      "11/11 - 0s - loss: 51.1604\n",
      "Epoch 4/150\n",
      "11/11 - 0s - loss: 49.9514\n",
      "Epoch 5/150\n",
      "11/11 - 0s - loss: 49.5188\n",
      "Epoch 6/150\n",
      "11/11 - 0s - loss: 49.3311\n",
      "Epoch 7/150\n",
      "11/11 - 0s - loss: 49.2091\n",
      "Epoch 8/150\n",
      "11/11 - 0s - loss: 48.8831\n",
      "Epoch 9/150\n",
      "11/11 - 0s - loss: 48.5027\n",
      "Epoch 10/150\n",
      "11/11 - 0s - loss: 48.8167\n",
      "Epoch 11/150\n",
      "11/11 - 0s - loss: 49.4790\n",
      "Epoch 12/150\n",
      "11/11 - 0s - loss: 48.2390\n",
      "Epoch 13/150\n",
      "11/11 - 0s - loss: 47.9045\n",
      "Epoch 14/150\n",
      "11/11 - 0s - loss: 48.0161\n",
      "Epoch 15/150\n",
      "11/11 - 0s - loss: 47.6609\n",
      "Epoch 16/150\n",
      "11/11 - 0s - loss: 48.5761\n",
      "Epoch 17/150\n",
      "11/11 - 0s - loss: 47.4882\n",
      "Epoch 18/150\n",
      "11/11 - 0s - loss: 46.6373\n",
      "Epoch 19/150\n",
      "11/11 - 0s - loss: 46.7964\n",
      "Epoch 20/150\n",
      "11/11 - 0s - loss: 46.8084\n",
      "Epoch 21/150\n",
      "11/11 - 0s - loss: 46.7562\n",
      "Epoch 22/150\n",
      "11/11 - 0s - loss: 45.9203\n",
      "Epoch 23/150\n",
      "11/11 - 0s - loss: 46.4801\n",
      "Epoch 24/150\n",
      "11/11 - 0s - loss: 46.5619\n",
      "Epoch 25/150\n",
      "11/11 - 0s - loss: 48.1595\n",
      "Epoch 26/150\n",
      "11/11 - 0s - loss: 46.8348\n",
      "Epoch 27/150\n",
      "11/11 - 0s - loss: 45.1234\n",
      "Epoch 28/150\n",
      "11/11 - 0s - loss: 46.2329\n",
      "Epoch 29/150\n",
      "11/11 - 0s - loss: 44.9371\n",
      "Epoch 30/150\n",
      "11/11 - 0s - loss: 44.6926\n",
      "Epoch 31/150\n",
      "11/11 - 0s - loss: 45.1607\n",
      "Epoch 32/150\n",
      "11/11 - 0s - loss: 44.0219\n",
      "Epoch 33/150\n",
      "11/11 - 0s - loss: 44.2540\n",
      "Epoch 34/150\n",
      "11/11 - 0s - loss: 45.1940\n",
      "Epoch 35/150\n",
      "11/11 - 0s - loss: 43.5900\n",
      "Epoch 36/150\n",
      "11/11 - 0s - loss: 43.4649\n",
      "Epoch 37/150\n",
      "11/11 - 0s - loss: 43.3235\n",
      "Epoch 38/150\n",
      "11/11 - 0s - loss: 43.4882\n",
      "Epoch 39/150\n",
      "11/11 - 0s - loss: 42.8637\n",
      "Epoch 40/150\n",
      "11/11 - 0s - loss: 43.0759\n",
      "Epoch 41/150\n",
      "11/11 - 0s - loss: 42.9808\n",
      "Epoch 42/150\n",
      "11/11 - 0s - loss: 42.3674\n",
      "Epoch 43/150\n",
      "11/11 - 0s - loss: 42.6051\n",
      "Epoch 44/150\n",
      "11/11 - 0s - loss: 42.0518\n",
      "Epoch 45/150\n",
      "11/11 - 0s - loss: 42.6070\n",
      "Epoch 46/150\n",
      "11/11 - 0s - loss: 41.5382\n",
      "Epoch 47/150\n",
      "11/11 - 0s - loss: 41.9439\n",
      "Epoch 48/150\n",
      "11/11 - 0s - loss: 42.2468\n",
      "Epoch 49/150\n",
      "11/11 - 0s - loss: 41.1752\n",
      "Epoch 50/150\n",
      "11/11 - 0s - loss: 41.7368\n",
      "Epoch 51/150\n",
      "11/11 - 0s - loss: 43.1026\n",
      "Epoch 52/150\n",
      "11/11 - 0s - loss: 41.7129\n",
      "Epoch 53/150\n",
      "11/11 - 0s - loss: 40.7693\n",
      "Epoch 54/150\n",
      "11/11 - 0s - loss: 41.2246\n",
      "Epoch 55/150\n",
      "11/11 - 0s - loss: 40.6391\n",
      "Epoch 56/150\n",
      "11/11 - 0s - loss: 40.6121\n",
      "Epoch 57/150\n",
      "11/11 - 0s - loss: 40.6106\n",
      "Epoch 58/150\n",
      "11/11 - 0s - loss: 41.6962\n",
      "Epoch 59/150\n",
      "11/11 - 0s - loss: 40.3410\n",
      "Epoch 60/150\n",
      "11/11 - 0s - loss: 40.8984\n",
      "Epoch 61/150\n",
      "11/11 - 0s - loss: 40.3868\n",
      "Epoch 62/150\n",
      "11/11 - 0s - loss: 39.1801\n",
      "Epoch 63/150\n",
      "11/11 - 0s - loss: 39.0151\n",
      "Epoch 64/150\n",
      "11/11 - 0s - loss: 40.5558\n",
      "Epoch 65/150\n",
      "11/11 - 0s - loss: 38.7678\n",
      "Epoch 66/150\n",
      "11/11 - 0s - loss: 39.8158\n",
      "Epoch 67/150\n",
      "11/11 - 0s - loss: 42.3536\n",
      "Epoch 68/150\n",
      "11/11 - 0s - loss: 38.7355\n",
      "Epoch 69/150\n",
      "11/11 - 0s - loss: 38.7226\n",
      "Epoch 70/150\n",
      "11/11 - 0s - loss: 39.3590\n",
      "Epoch 71/150\n",
      "11/11 - 0s - loss: 37.9208\n",
      "Epoch 72/150\n",
      "11/11 - 0s - loss: 37.9446\n",
      "Epoch 73/150\n",
      "11/11 - 0s - loss: 38.1186\n",
      "Epoch 74/150\n",
      "11/11 - 0s - loss: 38.0899\n",
      "Epoch 75/150\n",
      "11/11 - 0s - loss: 37.5969\n",
      "Epoch 76/150\n",
      "11/11 - 0s - loss: 37.3346\n",
      "Epoch 77/150\n",
      "11/11 - 0s - loss: 37.3972\n",
      "Epoch 78/150\n",
      "11/11 - 0s - loss: 37.6546\n",
      "Epoch 79/150\n",
      "11/11 - 0s - loss: 37.2855\n",
      "Epoch 80/150\n",
      "11/11 - 0s - loss: 38.6677\n",
      "Epoch 81/150\n",
      "11/11 - 0s - loss: 38.6018\n",
      "Epoch 82/150\n",
      "11/11 - 0s - loss: 40.9146\n",
      "Epoch 83/150\n",
      "11/11 - 0s - loss: 37.1305\n",
      "Epoch 84/150\n",
      "11/11 - 0s - loss: 37.2689\n",
      "Epoch 85/150\n",
      "11/11 - 0s - loss: 38.2660\n",
      "Epoch 86/150\n",
      "11/11 - 0s - loss: 36.7593\n",
      "Epoch 87/150\n",
      "11/11 - 0s - loss: 36.1595\n",
      "Epoch 88/150\n",
      "11/11 - 0s - loss: 38.2161\n",
      "Epoch 89/150\n",
      "11/11 - 0s - loss: 37.9513\n",
      "Epoch 90/150\n",
      "11/11 - 0s - loss: 36.7234\n",
      "Epoch 91/150\n",
      "11/11 - 0s - loss: 36.1040\n",
      "Epoch 92/150\n",
      "11/11 - 0s - loss: 36.0516\n",
      "Epoch 93/150\n",
      "11/11 - 0s - loss: 36.2924\n",
      "Epoch 94/150\n",
      "11/11 - 0s - loss: 35.6385\n",
      "Epoch 95/150\n",
      "11/11 - 0s - loss: 38.7609\n",
      "Epoch 96/150\n",
      "11/11 - 0s - loss: 36.6661\n",
      "Epoch 97/150\n",
      "11/11 - 0s - loss: 37.7457\n",
      "Epoch 98/150\n",
      "11/11 - 0s - loss: 37.1994\n",
      "Epoch 99/150\n",
      "11/11 - 0s - loss: 37.5731\n",
      "Epoch 100/150\n",
      "11/11 - 0s - loss: 38.8091\n",
      "Epoch 101/150\n",
      "11/11 - 0s - loss: 35.9699\n",
      "Epoch 102/150\n",
      "11/11 - 0s - loss: 37.1005\n",
      "Epoch 103/150\n",
      "11/11 - 0s - loss: 35.9884\n",
      "Epoch 104/150\n",
      "11/11 - 0s - loss: 35.0393\n",
      "Epoch 105/150\n",
      "11/11 - 0s - loss: 35.1893\n",
      "Epoch 106/150\n",
      "11/11 - 0s - loss: 35.8290\n",
      "Epoch 107/150\n",
      "11/11 - 0s - loss: 35.1375\n",
      "Epoch 108/150\n",
      "11/11 - 0s - loss: 35.0171\n",
      "Epoch 109/150\n",
      "11/11 - 0s - loss: 35.2656\n",
      "Epoch 110/150\n",
      "11/11 - 0s - loss: 37.5844\n",
      "Epoch 111/150\n",
      "11/11 - 0s - loss: 34.5924\n",
      "Epoch 112/150\n",
      "11/11 - 0s - loss: 35.1327\n",
      "Epoch 113/150\n",
      "11/11 - 0s - loss: 35.8955\n",
      "Epoch 114/150\n",
      "11/11 - 0s - loss: 34.0667\n",
      "Epoch 115/150\n",
      "11/11 - 0s - loss: 35.6867\n",
      "Epoch 116/150\n",
      "11/11 - 0s - loss: 34.8109\n",
      "Epoch 117/150\n",
      "11/11 - 0s - loss: 35.0200\n",
      "Epoch 118/150\n",
      "11/11 - 0s - loss: 35.2206\n",
      "Epoch 119/150\n",
      "11/11 - 0s - loss: 34.6397\n",
      "Epoch 120/150\n",
      "11/11 - 0s - loss: 34.4889\n",
      "Epoch 121/150\n",
      "11/11 - 0s - loss: 35.8072\n",
      "Epoch 122/150\n",
      "11/11 - 0s - loss: 35.9997\n",
      "Epoch 123/150\n",
      "11/11 - 0s - loss: 34.5704\n",
      "Epoch 124/150\n",
      "11/11 - 0s - loss: 34.1095\n",
      "Epoch 125/150\n",
      "11/11 - 0s - loss: 34.1565\n",
      "Epoch 126/150\n",
      "11/11 - 0s - loss: 34.5145\n",
      "Epoch 127/150\n",
      "11/11 - 0s - loss: 34.1861\n",
      "Epoch 128/150\n",
      "11/11 - 0s - loss: 34.3509\n",
      "Epoch 129/150\n",
      "11/11 - 0s - loss: 36.1554\n",
      "Epoch 130/150\n",
      "11/11 - 0s - loss: 35.7694\n",
      "Epoch 131/150\n",
      "11/11 - 0s - loss: 33.6842\n",
      "Epoch 132/150\n",
      "11/11 - 0s - loss: 33.8732\n",
      "Epoch 133/150\n",
      "11/11 - 0s - loss: 34.7760\n",
      "Epoch 134/150\n",
      "11/11 - 0s - loss: 34.7765\n",
      "Epoch 135/150\n",
      "11/11 - 0s - loss: 33.8543\n",
      "Epoch 136/150\n",
      "11/11 - 0s - loss: 33.9549\n",
      "Epoch 137/150\n",
      "11/11 - 0s - loss: 34.0057\n",
      "Epoch 138/150\n",
      "11/11 - 0s - loss: 33.7595\n",
      "Epoch 139/150\n",
      "11/11 - 0s - loss: 34.4036\n",
      "Epoch 140/150\n",
      "11/11 - 0s - loss: 34.9578\n",
      "Epoch 141/150\n",
      "11/11 - 0s - loss: 33.8313\n",
      "Epoch 142/150\n",
      "11/11 - 0s - loss: 33.4145\n",
      "Epoch 143/150\n",
      "11/11 - 0s - loss: 33.2758\n",
      "Epoch 144/150\n",
      "11/11 - 0s - loss: 33.2960\n",
      "Epoch 145/150\n",
      "11/11 - 0s - loss: 33.8109\n",
      "Epoch 146/150\n",
      "11/11 - 0s - loss: 33.8031\n",
      "Epoch 147/150\n",
      "11/11 - 0s - loss: 33.6301\n",
      "Epoch 148/150\n",
      "11/11 - 0s - loss: 32.9808\n",
      "Epoch 149/150\n",
      "11/11 - 0s - loss: 35.9067\n",
      "Epoch 150/150\n",
      "11/11 - 0s - loss: 33.6250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x12b5d4855e0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 38.516, RMSE: 6.206\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "error = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (error, sqrt(error)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mean Squared Error (MSE) is a measure of how close a fitted line is to data points,       \n",
    "Root Mean Squared Error (RMSE) is just the square root of the mean square error.        \n",
    "the lower the value the better and 0 means the model is perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 30.50\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "row = [0.00632,18.00,2.310,0,0.5380,6.5750,65.20,4.0900,1,296.0,15.30,396.90,4.98]\n",
    "yhat = model.predict([row])\n",
    "print('Predicted: %.2f' % yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks\n",
    "\n",
    "They are comprised of models with convolutional layers that extract features (called feature maps) and pooling layers that distill features down to the most salient elements.           \n",
    "\n",
    "CNNs are most well-suited to image classification tasks, although they can be used on a wide array of tasks that take images as input.            \n",
    "\n",
    "A popular image classification task is the MNIST handwritten digit classification. It involves tens of thousands of handwritten digits that must be classified as a number between 0 and 9.           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(60000, 28, 28), y=(60000,)\n",
      "Test: X=(10000, 28, 28), y=(10000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example of loading and plotting the mnist dataset\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load dataset\n",
    "(trainX, trainy), (testX, testy) = load_data()\n",
    "\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
    "\n",
    "# plot first few images\n",
    "for i in range(25):\n",
    "    # define subplot\n",
    "    pyplot.subplot(5, 5, i+1) \n",
    "    # plot raw pixel data \n",
    "    pyplot.imshow(trainX[i], cmap=pyplot.get_cmap('gray'))\n",
    "\n",
    "    \n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1) 10\n",
      "313/313 - 1s - loss: 0.0458 - accuracy: 0.9857\n",
      "Accuracy: 0.986\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000012B5E4CF820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Predicted: class=2\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "from numpy import asarray\n",
    "from numpy import unique\n",
    "from numpy import argmax\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "# reshape data to have a single channel: Vectorization: [rows, columns, channels];\n",
    "#where channels represent the color channels of the image data.\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], x_train.shape[2], 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], x_test.shape[2], 1))\n",
    "\n",
    "# determine the shape of the input images\n",
    "in_shape = x_train.shape[1:]\n",
    "\n",
    "# determine the number of classes\n",
    "n_classes = len(unique(y_train))\n",
    "print(in_shape, n_classes)\n",
    "\n",
    "# normalize pixel values\n",
    "#a good idea to scale the pixel values from the default range of 0-255 to 0-1 when training a CNN.\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', kernel_initializer='he_uniform', input_shape=in_shape))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "# define loss and optimizer\n",
    "model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "# make a prediction\n",
    "image = x_train[5]\n",
    "yhat = model.predict(asarray([image]))\n",
    "print('Predicted: class=%d' % argmax(yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks            \n",
    "Designed to operate upon sequences of data.             \n",
    "Very effective for natural language processing problems, time series forecasting and speech recognition.                        \n",
    "The most popular type of RNN is the Long Short-Term Memory network, or LSTM for short. LSTMs can be used in a model to accept a sequence of input data and make a prediction, such as assign a class label or predict a numerical value like the next value or values in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use : Monthly car sales in Quebec (1960-1968) Dataset\n",
    "\n",
    "Source: Time Series Data Library (citing: Abraham & Ledolter (1983))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 5, 1) (12, 5, 1) (91,) (12,)\n",
      "Epoch 1/350\n",
      "3/3 - 2s - loss: 1828820224.0000 - mae: 41084.0273 - val_loss: 1317611776.0000 - val_mae: 35799.0039\n",
      "Epoch 2/350\n",
      "3/3 - 0s - loss: 549706432.0000 - mae: 21858.1074 - val_loss: 190334528.0000 - val_mae: 13216.3525\n",
      "Epoch 3/350\n",
      "3/3 - 0s - loss: 75880312.0000 - mae: 7233.6655 - val_loss: 72553912.0000 - val_mae: 7026.2266\n",
      "Epoch 4/350\n",
      "3/3 - 0s - loss: 112743648.0000 - mae: 8663.2676 - val_loss: 271208032.0000 - val_mae: 15250.2822\n",
      "Epoch 5/350\n",
      "3/3 - 0s - loss: 200480480.0000 - mae: 12801.1143 - val_loss: 258089776.0000 - val_mae: 15109.5049\n",
      "Epoch 6/350\n",
      "3/3 - 0s - loss: 157399024.0000 - mae: 11193.8711 - val_loss: 130417376.0000 - val_mae: 10450.0029\n",
      "Epoch 7/350\n",
      "3/3 - 0s - loss: 75968784.0000 - mae: 7251.9463 - val_loss: 43883296.0000 - val_mae: 5995.1929\n",
      "Epoch 8/350\n",
      "3/3 - 0s - loss: 25329326.0000 - mae: 4040.7717 - val_loss: 16972698.0000 - val_mae: 2927.9597\n",
      "Epoch 9/350\n",
      "3/3 - 0s - loss: 16337806.0000 - mae: 3115.8203 - val_loss: 32602726.0000 - val_mae: 4271.5483\n",
      "Epoch 10/350\n",
      "3/3 - 0s - loss: 28650856.0000 - mae: 4391.7183 - val_loss: 49651364.0000 - val_mae: 5789.0317\n",
      "Epoch 11/350\n",
      "3/3 - 0s - loss: 35812944.0000 - mae: 5059.1753 - val_loss: 47971168.0000 - val_mae: 5655.1265\n",
      "Epoch 12/350\n",
      "3/3 - 0s - loss: 30608880.0000 - mae: 4605.0181 - val_loss: 25684398.0000 - val_mae: 4034.6360\n",
      "Epoch 13/350\n",
      "3/3 - 0s - loss: 19824668.0000 - mae: 3603.3462 - val_loss: 14542936.0000 - val_mae: 2959.5598\n",
      "Epoch 14/350\n",
      "3/3 - 0s - loss: 11267511.0000 - mae: 2608.4714 - val_loss: 13160793.0000 - val_mae: 3043.3396\n",
      "Epoch 15/350\n",
      "3/3 - 0s - loss: 11796517.0000 - mae: 2783.0598 - val_loss: 16902278.0000 - val_mae: 3517.6667\n",
      "Epoch 16/350\n",
      "3/3 - 0s - loss: 14510031.0000 - mae: 3196.9141 - val_loss: 19022366.0000 - val_mae: 3689.1912\n",
      "Epoch 17/350\n",
      "3/3 - 0s - loss: 15071771.0000 - mae: 3224.2913 - val_loss: 16220315.0000 - val_mae: 3408.2676\n",
      "Epoch 18/350\n",
      "3/3 - 0s - loss: 13719652.0000 - mae: 3037.0134 - val_loss: 13434133.0000 - val_mae: 3028.0957\n",
      "Epoch 19/350\n",
      "3/3 - 0s - loss: 11155870.0000 - mae: 2586.7930 - val_loss: 13689592.0000 - val_mae: 2981.2615\n",
      "Epoch 20/350\n",
      "3/3 - 0s - loss: 10998788.0000 - mae: 2493.1384 - val_loss: 14627172.0000 - val_mae: 2961.2170\n",
      "Epoch 21/350\n",
      "3/3 - 0s - loss: 10848325.0000 - mae: 2482.7705 - val_loss: 14978315.0000 - val_mae: 2964.1387\n",
      "Epoch 22/350\n",
      "3/3 - 0s - loss: 10934781.0000 - mae: 2496.0293 - val_loss: 13890163.0000 - val_mae: 2907.3074\n",
      "Epoch 23/350\n",
      "3/3 - 0s - loss: 10349741.0000 - mae: 2456.7271 - val_loss: 12531872.0000 - val_mae: 2848.9500\n",
      "Epoch 24/350\n",
      "3/3 - 0s - loss: 9757372.0000 - mae: 2436.4810 - val_loss: 12472832.0000 - val_mae: 2802.1311\n",
      "Epoch 25/350\n",
      "3/3 - 0s - loss: 9773735.0000 - mae: 2492.3259 - val_loss: 13025191.0000 - val_mae: 2904.1511\n",
      "Epoch 26/350\n",
      "3/3 - 0s - loss: 10041438.0000 - mae: 2564.9407 - val_loss: 13324912.0000 - val_mae: 2902.3367\n",
      "Epoch 27/350\n",
      "3/3 - 0s - loss: 10334077.0000 - mae: 2553.0188 - val_loss: 20473874.0000 - val_mae: 3367.8235\n",
      "Epoch 28/350\n",
      "3/3 - 0s - loss: 11493638.0000 - mae: 2648.9932 - val_loss: 14947559.0000 - val_mae: 2746.0457\n",
      "Epoch 29/350\n",
      "3/3 - 0s - loss: 12737038.0000 - mae: 2794.5979 - val_loss: 14532521.0000 - val_mae: 2743.0315\n",
      "Epoch 30/350\n",
      "3/3 - 0s - loss: 12602177.0000 - mae: 2775.9807 - val_loss: 15138128.0000 - val_mae: 2976.2620\n",
      "Epoch 31/350\n",
      "3/3 - 0s - loss: 13053007.0000 - mae: 2831.8735 - val_loss: 13986756.0000 - val_mae: 2900.9080\n",
      "Epoch 32/350\n",
      "3/3 - 0s - loss: 10995681.0000 - mae: 2606.8008 - val_loss: 21054712.0000 - val_mae: 3660.5566\n",
      "Epoch 33/350\n",
      "3/3 - 0s - loss: 10343892.0000 - mae: 2581.5320 - val_loss: 14237325.0000 - val_mae: 3074.5315\n",
      "Epoch 34/350\n",
      "3/3 - 0s - loss: 10030423.0000 - mae: 2530.4048 - val_loss: 12775120.0000 - val_mae: 2796.9695\n",
      "Epoch 35/350\n",
      "3/3 - 0s - loss: 10256109.0000 - mae: 2569.8359 - val_loss: 12365161.0000 - val_mae: 2784.1318\n",
      "Epoch 36/350\n",
      "3/3 - 0s - loss: 9935794.0000 - mae: 2481.0066 - val_loss: 12200023.0000 - val_mae: 2787.8821\n",
      "Epoch 37/350\n",
      "3/3 - 0s - loss: 9169653.0000 - mae: 2368.6270 - val_loss: 12280733.0000 - val_mae: 2801.6973\n",
      "Epoch 38/350\n",
      "3/3 - 0s - loss: 9243121.0000 - mae: 2352.4744 - val_loss: 12228276.0000 - val_mae: 2792.3750\n",
      "Epoch 39/350\n",
      "3/3 - 0s - loss: 9079541.0000 - mae: 2322.6133 - val_loss: 12133493.0000 - val_mae: 2774.6553\n",
      "Epoch 40/350\n",
      "3/3 - 0s - loss: 9086099.0000 - mae: 2356.6553 - val_loss: 12087749.0000 - val_mae: 2756.0840\n",
      "Epoch 41/350\n",
      "3/3 - 0s - loss: 8913170.0000 - mae: 2341.2656 - val_loss: 12061473.0000 - val_mae: 2748.0061\n",
      "Epoch 42/350\n",
      "3/3 - 0s - loss: 9040141.0000 - mae: 2378.7280 - val_loss: 12862691.0000 - val_mae: 2921.9431\n",
      "Epoch 43/350\n",
      "3/3 - 0s - loss: 10808472.0000 - mae: 2619.5144 - val_loss: 12824723.0000 - val_mae: 2924.9871\n",
      "Epoch 44/350\n",
      "3/3 - 0s - loss: 11863425.0000 - mae: 2713.1885 - val_loss: 15089549.0000 - val_mae: 3141.2605\n",
      "Epoch 45/350\n",
      "3/3 - 0s - loss: 11248809.0000 - mae: 2627.7363 - val_loss: 14814209.0000 - val_mae: 3216.1624\n",
      "Epoch 46/350\n",
      "3/3 - 0s - loss: 10819114.0000 - mae: 2556.0420 - val_loss: 14023709.0000 - val_mae: 3263.8469\n",
      "Epoch 47/350\n",
      "3/3 - 0s - loss: 10905515.0000 - mae: 2613.3550 - val_loss: 14840152.0000 - val_mae: 3312.9053\n",
      "Epoch 48/350\n",
      "3/3 - 0s - loss: 10890971.0000 - mae: 2594.3638 - val_loss: 14755893.0000 - val_mae: 3305.6355\n",
      "Epoch 49/350\n",
      "3/3 - 0s - loss: 11970480.0000 - mae: 2681.2427 - val_loss: 13467735.0000 - val_mae: 3187.8347\n",
      "Epoch 50/350\n",
      "3/3 - 0s - loss: 12268047.0000 - mae: 2745.6555 - val_loss: 13116227.0000 - val_mae: 3110.0798\n",
      "Epoch 51/350\n",
      "3/3 - 0s - loss: 11420537.0000 - mae: 2687.9685 - val_loss: 13053711.0000 - val_mae: 3109.3176\n",
      "Epoch 52/350\n",
      "3/3 - 0s - loss: 11824660.0000 - mae: 2761.7317 - val_loss: 13275576.0000 - val_mae: 3192.0686\n",
      "Epoch 53/350\n",
      "3/3 - 0s - loss: 11765335.0000 - mae: 2777.3401 - val_loss: 13151628.0000 - val_mae: 3191.4626\n",
      "Epoch 54/350\n",
      "3/3 - 0s - loss: 12397381.0000 - mae: 2811.5842 - val_loss: 13140504.0000 - val_mae: 3165.9121\n",
      "Epoch 55/350\n",
      "3/3 - 0s - loss: 12314172.0000 - mae: 2798.9934 - val_loss: 13262677.0000 - val_mae: 3132.9971\n",
      "Epoch 56/350\n",
      "3/3 - 0s - loss: 12232068.0000 - mae: 2767.9653 - val_loss: 13291408.0000 - val_mae: 3143.2939\n",
      "Epoch 57/350\n",
      "3/3 - 0s - loss: 12124559.0000 - mae: 2751.2363 - val_loss: 13315672.0000 - val_mae: 3140.8132\n",
      "Epoch 58/350\n",
      "3/3 - 0s - loss: 11931724.0000 - mae: 2728.7351 - val_loss: 13401492.0000 - val_mae: 3115.3132\n",
      "Epoch 59/350\n",
      "3/3 - 0s - loss: 11876583.0000 - mae: 2718.9177 - val_loss: 13357904.0000 - val_mae: 3118.3079\n",
      "Epoch 60/350\n",
      "3/3 - 0s - loss: 11826311.0000 - mae: 2706.3064 - val_loss: 13274528.0000 - val_mae: 3131.4988\n",
      "Epoch 61/350\n",
      "3/3 - 0s - loss: 11723264.0000 - mae: 2695.9124 - val_loss: 13174819.0000 - val_mae: 3143.2180\n",
      "Epoch 62/350\n",
      "3/3 - 0s - loss: 11699689.0000 - mae: 2692.2607 - val_loss: 13151476.0000 - val_mae: 3138.2195\n",
      "Epoch 63/350\n",
      "3/3 - 0s - loss: 11646032.0000 - mae: 2686.5251 - val_loss: 13003376.0000 - val_mae: 3185.5754\n",
      "Epoch 64/350\n",
      "3/3 - 0s - loss: 11570744.0000 - mae: 2680.6089 - val_loss: 12631199.0000 - val_mae: 3043.1975\n",
      "Epoch 65/350\n",
      "3/3 - 0s - loss: 11474494.0000 - mae: 2672.9270 - val_loss: 12633597.0000 - val_mae: 3008.6140\n",
      "Epoch 66/350\n",
      "3/3 - 0s - loss: 11430822.0000 - mae: 2668.7422 - val_loss: 12712097.0000 - val_mae: 2932.7952\n",
      "Epoch 67/350\n",
      "3/3 - 0s - loss: 11486525.0000 - mae: 2671.9351 - val_loss: 12590487.0000 - val_mae: 2969.1016\n",
      "Epoch 68/350\n",
      "3/3 - 0s - loss: 11358170.0000 - mae: 2653.1919 - val_loss: 13039871.0000 - val_mae: 3103.5977\n",
      "Epoch 69/350\n",
      "3/3 - 0s - loss: 11462020.0000 - mae: 2664.8574 - val_loss: 12928711.0000 - val_mae: 3124.7227\n",
      "Epoch 70/350\n",
      "3/3 - 0s - loss: 10569080.0000 - mae: 2584.9487 - val_loss: 12872803.0000 - val_mae: 3140.7898\n",
      "Epoch 71/350\n",
      "3/3 - 0s - loss: 10691894.0000 - mae: 2596.6692 - val_loss: 12892068.0000 - val_mae: 3141.6140\n",
      "Epoch 72/350\n",
      "3/3 - 0s - loss: 10583456.0000 - mae: 2580.3879 - val_loss: 12881812.0000 - val_mae: 3156.6365\n",
      "Epoch 73/350\n",
      "3/3 - 0s - loss: 10680888.0000 - mae: 2587.0378 - val_loss: 12848643.0000 - val_mae: 3185.4783\n",
      "Epoch 74/350\n",
      "3/3 - 0s - loss: 10641793.0000 - mae: 2582.7222 - val_loss: 12910141.0000 - val_mae: 3166.4465\n",
      "Epoch 75/350\n",
      "3/3 - 0s - loss: 10556799.0000 - mae: 2569.6367 - val_loss: 12958040.0000 - val_mae: 3163.9485\n",
      "Epoch 76/350\n",
      "3/3 - 0s - loss: 10527853.0000 - mae: 2563.4949 - val_loss: 13144464.0000 - val_mae: 3154.2844\n",
      "Epoch 77/350\n",
      "3/3 - 0s - loss: 10410312.0000 - mae: 2543.2178 - val_loss: 13131081.0000 - val_mae: 3158.4648\n",
      "Epoch 78/350\n",
      "3/3 - 0s - loss: 10389398.0000 - mae: 2534.2646 - val_loss: 12966871.0000 - val_mae: 3178.1094\n",
      "Epoch 79/350\n",
      "3/3 - 0s - loss: 10296611.0000 - mae: 2529.3489 - val_loss: 12963835.0000 - val_mae: 3187.3093\n",
      "Epoch 80/350\n",
      "3/3 - 0s - loss: 10270844.0000 - mae: 2526.7139 - val_loss: 12964512.0000 - val_mae: 3190.7527\n",
      "Epoch 81/350\n",
      "3/3 - 0s - loss: 10305355.0000 - mae: 2527.5557 - val_loss: 12965280.0000 - val_mae: 3189.2488\n",
      "Epoch 82/350\n",
      "3/3 - 0s - loss: 10147909.0000 - mae: 2504.0879 - val_loss: 13138317.0000 - val_mae: 3165.5413\n",
      "Epoch 83/350\n",
      "3/3 - 0s - loss: 10185504.0000 - mae: 2506.4695 - val_loss: 13195136.0000 - val_mae: 3163.6238\n",
      "Epoch 84/350\n",
      "3/3 - 0s - loss: 10138908.0000 - mae: 2498.4321 - val_loss: 13057196.0000 - val_mae: 3165.4131\n",
      "Epoch 85/350\n",
      "3/3 - 0s - loss: 10063672.0000 - mae: 2482.2964 - val_loss: 12917483.0000 - val_mae: 3167.9736\n",
      "Epoch 86/350\n",
      "3/3 - 0s - loss: 10020896.0000 - mae: 2477.3176 - val_loss: 12811712.0000 - val_mae: 3167.1111\n",
      "Epoch 87/350\n",
      "3/3 - 0s - loss: 9930663.0000 - mae: 2465.8726 - val_loss: 12766205.0000 - val_mae: 3178.1399\n",
      "Epoch 88/350\n",
      "3/3 - 0s - loss: 10012055.0000 - mae: 2482.7698 - val_loss: 12795115.0000 - val_mae: 3212.0020\n",
      "Epoch 89/350\n",
      "3/3 - 0s - loss: 9845560.0000 - mae: 2465.4883 - val_loss: 12855827.0000 - val_mae: 3160.7180\n",
      "Epoch 90/350\n",
      "3/3 - 0s - loss: 9738544.0000 - mae: 2433.9749 - val_loss: 13029517.0000 - val_mae: 3137.1416\n",
      "Epoch 91/350\n",
      "3/3 - 0s - loss: 9742598.0000 - mae: 2438.0710 - val_loss: 13079051.0000 - val_mae: 3125.7168\n",
      "Epoch 92/350\n",
      "3/3 - 0s - loss: 9755572.0000 - mae: 2435.3623 - val_loss: 13019535.0000 - val_mae: 3131.0339\n",
      "Epoch 93/350\n",
      "3/3 - 0s - loss: 9638961.0000 - mae: 2423.3850 - val_loss: 12786109.0000 - val_mae: 3175.5530\n",
      "Epoch 94/350\n",
      "3/3 - 0s - loss: 9584705.0000 - mae: 2414.6953 - val_loss: 12742113.0000 - val_mae: 3198.9180\n",
      "Epoch 95/350\n",
      "3/3 - 0s - loss: 9583733.0000 - mae: 2416.8574 - val_loss: 12644088.0000 - val_mae: 3169.5715\n",
      "Epoch 96/350\n",
      "3/3 - 0s - loss: 9506050.0000 - mae: 2398.8684 - val_loss: 12328524.0000 - val_mae: 3007.7898\n",
      "Epoch 97/350\n",
      "3/3 - 0s - loss: 9430743.0000 - mae: 2393.0437 - val_loss: 12301221.0000 - val_mae: 2953.2590\n",
      "Epoch 98/350\n",
      "3/3 - 0s - loss: 9423667.0000 - mae: 2391.8059 - val_loss: 12239955.0000 - val_mae: 2965.7109\n",
      "Epoch 99/350\n",
      "3/3 - 0s - loss: 9391825.0000 - mae: 2384.7300 - val_loss: 12262331.0000 - val_mae: 2915.5320\n",
      "Epoch 100/350\n",
      "3/3 - 0s - loss: 9299540.0000 - mae: 2376.0234 - val_loss: 12109448.0000 - val_mae: 2984.5652\n",
      "Epoch 101/350\n",
      "3/3 - 0s - loss: 9205954.0000 - mae: 2368.9512 - val_loss: 11982712.0000 - val_mae: 3003.6348\n",
      "Epoch 102/350\n",
      "3/3 - 0s - loss: 9136243.0000 - mae: 2354.3276 - val_loss: 11868644.0000 - val_mae: 2908.0886\n",
      "Epoch 103/350\n",
      "3/3 - 0s - loss: 8969136.0000 - mae: 2335.8069 - val_loss: 11816019.0000 - val_mae: 2904.1367\n",
      "Epoch 104/350\n",
      "3/3 - 0s - loss: 8874367.0000 - mae: 2323.4893 - val_loss: 11784368.0000 - val_mae: 2960.0742\n",
      "Epoch 105/350\n",
      "3/3 - 0s - loss: 8696161.0000 - mae: 2312.0486 - val_loss: 11734763.0000 - val_mae: 2924.3933\n",
      "Epoch 106/350\n",
      "3/3 - 0s - loss: 8598092.0000 - mae: 2301.1799 - val_loss: 11701724.0000 - val_mae: 2866.5471\n",
      "Epoch 107/350\n",
      "3/3 - 0s - loss: 8557531.0000 - mae: 2295.6133 - val_loss: 11664375.0000 - val_mae: 2883.3225\n",
      "Epoch 108/350\n",
      "3/3 - 0s - loss: 8528070.0000 - mae: 2298.4714 - val_loss: 11650068.0000 - val_mae: 2874.7676\n",
      "Epoch 109/350\n",
      "3/3 - 0s - loss: 8495989.0000 - mae: 2289.7109 - val_loss: 11650296.0000 - val_mae: 2851.5369\n",
      "Epoch 110/350\n",
      "3/3 - 0s - loss: 8423693.0000 - mae: 2271.5125 - val_loss: 11625057.0000 - val_mae: 2846.9290\n",
      "Epoch 111/350\n",
      "3/3 - 0s - loss: 8383728.5000 - mae: 2265.3042 - val_loss: 11580153.0000 - val_mae: 2870.2754\n",
      "Epoch 112/350\n",
      "3/3 - 0s - loss: 8376746.0000 - mae: 2279.0720 - val_loss: 11570084.0000 - val_mae: 2905.9124\n",
      "Epoch 113/350\n",
      "3/3 - 0s - loss: 8347770.5000 - mae: 2286.6016 - val_loss: 11563835.0000 - val_mae: 2908.6501\n",
      "Epoch 114/350\n",
      "3/3 - 0s - loss: 8262165.0000 - mae: 2264.8774 - val_loss: 11591892.0000 - val_mae: 2853.9143\n",
      "Epoch 115/350\n",
      "3/3 - 0s - loss: 8275479.0000 - mae: 2253.1201 - val_loss: 11602224.0000 - val_mae: 2848.5225\n",
      "Epoch 116/350\n",
      "3/3 - 0s - loss: 8250017.0000 - mae: 2248.6902 - val_loss: 11623500.0000 - val_mae: 2833.7031\n",
      "Epoch 117/350\n",
      "3/3 - 0s - loss: 8249826.5000 - mae: 2259.0093 - val_loss: 11562767.0000 - val_mae: 2900.8328\n",
      "Epoch 118/350\n",
      "3/3 - 0s - loss: 8227652.0000 - mae: 2270.3445 - val_loss: 11567243.0000 - val_mae: 2923.3616\n",
      "Epoch 119/350\n",
      "3/3 - 0s - loss: 8149568.0000 - mae: 2256.8167 - val_loss: 11556172.0000 - val_mae: 2853.3701\n",
      "Epoch 120/350\n",
      "3/3 - 0s - loss: 8152054.0000 - mae: 2244.2949 - val_loss: 11561288.0000 - val_mae: 2840.3770\n",
      "Epoch 121/350\n",
      "3/3 - 0s - loss: 8072751.0000 - mae: 2231.7629 - val_loss: 11518663.0000 - val_mae: 2884.4861\n",
      "Epoch 122/350\n",
      "3/3 - 0s - loss: 8056102.0000 - mae: 2247.5657 - val_loss: 11552929.0000 - val_mae: 2917.8740\n",
      "Epoch 123/350\n",
      "3/3 - 0s - loss: 8057594.5000 - mae: 2256.8325 - val_loss: 11555649.0000 - val_mae: 2905.3411\n",
      "Epoch 124/350\n",
      "3/3 - 0s - loss: 7960844.5000 - mae: 2232.7302 - val_loss: 11527463.0000 - val_mae: 2850.9922\n",
      "Epoch 125/350\n",
      "3/3 - 0s - loss: 7945256.0000 - mae: 2216.8713 - val_loss: 11539363.0000 - val_mae: 2834.3281\n",
      "Epoch 126/350\n",
      "3/3 - 0s - loss: 7929027.0000 - mae: 2216.5557 - val_loss: 11510004.0000 - val_mae: 2858.8806\n",
      "Epoch 127/350\n",
      "3/3 - 0s - loss: 7971505.0000 - mae: 2221.1450 - val_loss: 11507777.0000 - val_mae: 2857.3894\n",
      "Epoch 128/350\n",
      "3/3 - 0s - loss: 7843018.0000 - mae: 2205.7388 - val_loss: 11602987.0000 - val_mae: 2934.5898\n",
      "Epoch 129/350\n",
      "3/3 - 0s - loss: 7904820.5000 - mae: 2245.3694 - val_loss: 11649437.0000 - val_mae: 2943.6072\n",
      "Epoch 130/350\n",
      "3/3 - 0s - loss: 7851174.0000 - mae: 2230.4736 - val_loss: 11511949.0000 - val_mae: 2857.8486\n",
      "Epoch 131/350\n",
      "3/3 - 0s - loss: 7819710.5000 - mae: 2209.3394 - val_loss: 11548911.0000 - val_mae: 2810.6438\n",
      "Epoch 132/350\n",
      "3/3 - 0s - loss: 7779316.5000 - mae: 2197.7646 - val_loss: 11507656.0000 - val_mae: 2853.7478\n",
      "Epoch 133/350\n",
      "3/3 - 0s - loss: 7728894.0000 - mae: 2208.7393 - val_loss: 11549720.0000 - val_mae: 2901.0410\n",
      "Epoch 134/350\n",
      "3/3 - 0s - loss: 7758362.0000 - mae: 2216.6260 - val_loss: 11580683.0000 - val_mae: 2915.0950\n",
      "Epoch 135/350\n",
      "3/3 - 0s - loss: 7686037.0000 - mae: 2208.8655 - val_loss: 11507784.0000 - val_mae: 2848.2566\n",
      "Epoch 136/350\n",
      "3/3 - 0s - loss: 7746457.5000 - mae: 2198.7134 - val_loss: 11561713.0000 - val_mae: 2797.6572\n",
      "Epoch 137/350\n",
      "3/3 - 0s - loss: 7725152.5000 - mae: 2192.7249 - val_loss: 11528143.0000 - val_mae: 2870.9189\n",
      "Epoch 138/350\n",
      "3/3 - 0s - loss: 7687389.0000 - mae: 2209.6665 - val_loss: 11606264.0000 - val_mae: 2920.0720\n",
      "Epoch 139/350\n",
      "3/3 - 0s - loss: 7616006.5000 - mae: 2200.7886 - val_loss: 11521489.0000 - val_mae: 2855.1194\n",
      "Epoch 140/350\n",
      "3/3 - 0s - loss: 7571655.5000 - mae: 2185.6091 - val_loss: 11520853.0000 - val_mae: 2835.3291\n",
      "Epoch 141/350\n",
      "3/3 - 0s - loss: 7568038.0000 - mae: 2179.1167 - val_loss: 11524219.0000 - val_mae: 2838.1230\n",
      "Epoch 142/350\n",
      "3/3 - 0s - loss: 7593353.0000 - mae: 2188.8098 - val_loss: 11552663.0000 - val_mae: 2872.8469\n",
      "Epoch 143/350\n",
      "3/3 - 0s - loss: 7514224.5000 - mae: 2183.8184 - val_loss: 11540740.0000 - val_mae: 2857.2751\n",
      "Epoch 144/350\n",
      "3/3 - 0s - loss: 7478431.0000 - mae: 2173.0957 - val_loss: 11526619.0000 - val_mae: 2831.5466\n",
      "Epoch 145/350\n",
      "3/3 - 0s - loss: 7488292.5000 - mae: 2167.6865 - val_loss: 11531744.0000 - val_mae: 2830.4470\n",
      "Epoch 146/350\n",
      "3/3 - 0s - loss: 7474448.0000 - mae: 2167.9646 - val_loss: 11557243.0000 - val_mae: 2871.8359\n",
      "Epoch 147/350\n",
      "3/3 - 0s - loss: 7431962.5000 - mae: 2172.7485 - val_loss: 11601323.0000 - val_mae: 2889.5664\n",
      "Epoch 148/350\n",
      "3/3 - 0s - loss: 7420097.5000 - mae: 2174.4800 - val_loss: 11615576.0000 - val_mae: 2890.1570\n",
      "Epoch 149/350\n",
      "3/3 - 0s - loss: 7408515.0000 - mae: 2175.6484 - val_loss: 11586720.0000 - val_mae: 2873.6218\n",
      "Epoch 150/350\n",
      "3/3 - 0s - loss: 7365458.5000 - mae: 2166.7493 - val_loss: 11563389.0000 - val_mae: 2845.3689\n",
      "Epoch 151/350\n",
      "3/3 - 0s - loss: 7474333.5000 - mae: 2173.1538 - val_loss: 11565208.0000 - val_mae: 2794.9670\n",
      "Epoch 152/350\n",
      "3/3 - 0s - loss: 7382907.0000 - mae: 2160.7146 - val_loss: 11601019.0000 - val_mae: 2881.3308\n",
      "Epoch 153/350\n",
      "3/3 - 0s - loss: 7338259.0000 - mae: 2164.8196 - val_loss: 11603352.0000 - val_mae: 2888.3391\n",
      "Epoch 154/350\n",
      "3/3 - 0s - loss: 7421654.5000 - mae: 2176.5022 - val_loss: 11722727.0000 - val_mae: 2925.0598\n",
      "Epoch 155/350\n",
      "3/3 - 0s - loss: 7371385.0000 - mae: 2164.2405 - val_loss: 11554320.0000 - val_mae: 2813.3250\n",
      "Epoch 156/350\n",
      "3/3 - 0s - loss: 7296752.5000 - mae: 2137.4497 - val_loss: 11552912.0000 - val_mae: 2791.1104\n",
      "Epoch 157/350\n",
      "3/3 - 0s - loss: 7266790.0000 - mae: 2139.4192 - val_loss: 11592753.0000 - val_mae: 2847.5144\n",
      "Epoch 158/350\n",
      "3/3 - 0s - loss: 7238436.5000 - mae: 2146.4307 - val_loss: 11655309.0000 - val_mae: 2872.6506\n",
      "Epoch 159/350\n",
      "3/3 - 0s - loss: 7279619.5000 - mae: 2160.3884 - val_loss: 11692988.0000 - val_mae: 2889.2156\n",
      "Epoch 160/350\n",
      "3/3 - 0s - loss: 7262666.5000 - mae: 2146.6370 - val_loss: 11554821.0000 - val_mae: 2797.1663\n",
      "Epoch 161/350\n",
      "3/3 - 0s - loss: 7207258.0000 - mae: 2128.4631 - val_loss: 11558632.0000 - val_mae: 2805.8938\n",
      "Epoch 162/350\n",
      "3/3 - 0s - loss: 7168752.5000 - mae: 2124.8296 - val_loss: 11540423.0000 - val_mae: 2815.7512\n",
      "Epoch 163/350\n",
      "3/3 - 0s - loss: 7160290.0000 - mae: 2132.7864 - val_loss: 11591017.0000 - val_mae: 2857.0881\n",
      "Epoch 164/350\n",
      "3/3 - 0s - loss: 7134121.0000 - mae: 2134.7490 - val_loss: 11620499.0000 - val_mae: 2858.1553\n",
      "Epoch 165/350\n",
      "3/3 - 0s - loss: 7144320.0000 - mae: 2132.3638 - val_loss: 11627021.0000 - val_mae: 2849.9619\n",
      "Epoch 166/350\n",
      "3/3 - 0s - loss: 7219305.5000 - mae: 2117.4636 - val_loss: 11593932.0000 - val_mae: 2770.9377\n",
      "Epoch 167/350\n",
      "3/3 - 0s - loss: 7109829.0000 - mae: 2109.0361 - val_loss: 11590133.0000 - val_mae: 2823.1562\n",
      "Epoch 168/350\n",
      "3/3 - 0s - loss: 7063791.0000 - mae: 2114.7556 - val_loss: 11715347.0000 - val_mae: 2873.5803\n",
      "Epoch 169/350\n",
      "3/3 - 0s - loss: 7120737.0000 - mae: 2125.4856 - val_loss: 11616285.0000 - val_mae: 2832.8430\n",
      "Epoch 170/350\n",
      "3/3 - 0s - loss: 7057804.5000 - mae: 2114.7305 - val_loss: 11632244.0000 - val_mae: 2838.5168\n",
      "Epoch 171/350\n",
      "3/3 - 0s - loss: 7180593.0000 - mae: 2139.7625 - val_loss: 11679853.0000 - val_mae: 2855.8552\n",
      "Epoch 172/350\n",
      "3/3 - 0s - loss: 7085623.0000 - mae: 2115.6741 - val_loss: 11590664.0000 - val_mae: 2786.9739\n",
      "Epoch 173/350\n",
      "3/3 - 0s - loss: 7074347.0000 - mae: 2101.0266 - val_loss: 11571744.0000 - val_mae: 2790.0811\n",
      "Epoch 174/350\n",
      "3/3 - 0s - loss: 7130569.0000 - mae: 2116.7178 - val_loss: 11528932.0000 - val_mae: 2805.7454\n",
      "Epoch 175/350\n",
      "3/3 - 0s - loss: 6987898.5000 - mae: 2102.7896 - val_loss: 11584892.0000 - val_mae: 2827.0032\n",
      "Epoch 176/350\n",
      "3/3 - 0s - loss: 7005753.5000 - mae: 2108.8354 - val_loss: 11589405.0000 - val_mae: 2829.5994\n",
      "Epoch 177/350\n",
      "3/3 - 0s - loss: 6931776.5000 - mae: 2098.5193 - val_loss: 11550724.0000 - val_mae: 2823.7883\n",
      "Epoch 178/350\n",
      "3/3 - 0s - loss: 6965373.0000 - mae: 2079.7036 - val_loss: 11467197.0000 - val_mae: 2770.3711\n",
      "Epoch 179/350\n",
      "3/3 - 0s - loss: 6931279.0000 - mae: 2078.6426 - val_loss: 11556333.0000 - val_mae: 2805.9443\n",
      "Epoch 180/350\n",
      "3/3 - 0s - loss: 6893936.0000 - mae: 2082.0286 - val_loss: 11556117.0000 - val_mae: 2797.3254\n",
      "Epoch 181/350\n",
      "3/3 - 0s - loss: 6888511.5000 - mae: 2079.9924 - val_loss: 11581624.0000 - val_mae: 2827.6221\n",
      "Epoch 182/350\n",
      "3/3 - 0s - loss: 6871998.5000 - mae: 2082.1768 - val_loss: 11491272.0000 - val_mae: 2791.9490\n",
      "Epoch 183/350\n",
      "3/3 - 0s - loss: 6861700.0000 - mae: 2077.3755 - val_loss: 11460467.0000 - val_mae: 2791.3359\n",
      "Epoch 184/350\n",
      "3/3 - 0s - loss: 6962739.5000 - mae: 2087.3018 - val_loss: 11427212.0000 - val_mae: 2773.1223\n",
      "Epoch 185/350\n",
      "3/3 - 0s - loss: 6800134.5000 - mae: 2058.7761 - val_loss: 11503129.0000 - val_mae: 2790.1318\n",
      "Epoch 186/350\n",
      "3/3 - 0s - loss: 6787316.5000 - mae: 2066.2021 - val_loss: 11599829.0000 - val_mae: 2820.1829\n",
      "Epoch 187/350\n",
      "3/3 - 0s - loss: 6818208.5000 - mae: 2085.5127 - val_loss: 11619036.0000 - val_mae: 2821.2705\n",
      "Epoch 188/350\n",
      "3/3 - 0s - loss: 6797286.5000 - mae: 2072.3984 - val_loss: 11451349.0000 - val_mae: 2769.6091\n",
      "Epoch 189/350\n",
      "3/3 - 0s - loss: 6773519.5000 - mae: 2050.0835 - val_loss: 11412499.0000 - val_mae: 2768.5891\n",
      "Epoch 190/350\n",
      "3/3 - 0s - loss: 6802739.5000 - mae: 2050.2573 - val_loss: 11412917.0000 - val_mae: 2769.2969\n",
      "Epoch 191/350\n",
      "3/3 - 0s - loss: 6756282.5000 - mae: 2063.5142 - val_loss: 11612203.0000 - val_mae: 2828.5352\n",
      "Epoch 192/350\n",
      "3/3 - 0s - loss: 6771546.0000 - mae: 2073.4221 - val_loss: 11491453.0000 - val_mae: 2788.0310\n",
      "Epoch 193/350\n",
      "3/3 - 0s - loss: 6764853.5000 - mae: 2059.6226 - val_loss: 11426248.0000 - val_mae: 2786.4871\n",
      "Epoch 194/350\n",
      "3/3 - 0s - loss: 6731184.5000 - mae: 2042.9045 - val_loss: 11440301.0000 - val_mae: 2778.8193\n",
      "Epoch 195/350\n",
      "3/3 - 0s - loss: 6710768.0000 - mae: 2054.2275 - val_loss: 11476373.0000 - val_mae: 2777.8245\n",
      "Epoch 196/350\n",
      "3/3 - 0s - loss: 6714364.0000 - mae: 2054.1794 - val_loss: 11460055.0000 - val_mae: 2775.1785\n",
      "Epoch 197/350\n",
      "3/3 - 0s - loss: 6699904.0000 - mae: 2048.9541 - val_loss: 11466416.0000 - val_mae: 2783.5615\n",
      "Epoch 198/350\n",
      "3/3 - 0s - loss: 6796224.0000 - mae: 2044.8624 - val_loss: 11443051.0000 - val_mae: 2797.8801\n",
      "Epoch 199/350\n",
      "3/3 - 0s - loss: 6805913.5000 - mae: 2047.2140 - val_loss: 11575277.0000 - val_mae: 2794.1614\n",
      "Epoch 200/350\n",
      "3/3 - 0s - loss: 6690515.5000 - mae: 2044.2820 - val_loss: 11418613.0000 - val_mae: 2769.1436\n",
      "Epoch 201/350\n",
      "3/3 - 0s - loss: 6743832.0000 - mae: 2041.3208 - val_loss: 11386716.0000 - val_mae: 2774.1990\n",
      "Epoch 202/350\n",
      "3/3 - 0s - loss: 6685387.5000 - mae: 2046.8369 - val_loss: 11537819.0000 - val_mae: 2784.1760\n",
      "Epoch 203/350\n",
      "3/3 - 0s - loss: 6663430.5000 - mae: 2052.3325 - val_loss: 11523704.0000 - val_mae: 2782.6746\n",
      "Epoch 204/350\n",
      "3/3 - 0s - loss: 6660263.5000 - mae: 2040.3243 - val_loss: 11443125.0000 - val_mae: 2791.5332\n",
      "Epoch 205/350\n",
      "3/3 - 0s - loss: 6651326.0000 - mae: 2028.4672 - val_loss: 11405511.0000 - val_mae: 2785.5193\n",
      "Epoch 206/350\n",
      "3/3 - 0s - loss: 6675865.5000 - mae: 2044.6428 - val_loss: 11504299.0000 - val_mae: 2787.9236\n",
      "Epoch 207/350\n",
      "3/3 - 0s - loss: 6803174.5000 - mae: 2067.3130 - val_loss: 11659184.0000 - val_mae: 2806.9934\n",
      "Epoch 208/350\n",
      "3/3 - 0s - loss: 6641158.5000 - mae: 2041.4264 - val_loss: 11370331.0000 - val_mae: 2785.5798\n",
      "Epoch 209/350\n",
      "3/3 - 0s - loss: 6673195.0000 - mae: 2014.1559 - val_loss: 11387192.0000 - val_mae: 2796.3157\n",
      "Epoch 210/350\n",
      "3/3 - 0s - loss: 6630258.0000 - mae: 2012.3214 - val_loss: 11403968.0000 - val_mae: 2781.2610\n",
      "Epoch 211/350\n",
      "3/3 - 0s - loss: 6629177.5000 - mae: 2051.4407 - val_loss: 11680237.0000 - val_mae: 2813.1179\n",
      "Epoch 212/350\n",
      "3/3 - 0s - loss: 6698345.5000 - mae: 2067.0837 - val_loss: 11394232.0000 - val_mae: 2781.8210\n",
      "Epoch 213/350\n",
      "3/3 - 0s - loss: 6589178.5000 - mae: 2022.7277 - val_loss: 11360873.0000 - val_mae: 2766.8552\n",
      "Epoch 214/350\n",
      "3/3 - 0s - loss: 6572431.5000 - mae: 2018.8107 - val_loss: 11383109.0000 - val_mae: 2768.5740\n",
      "Epoch 215/350\n",
      "3/3 - 0s - loss: 6570620.0000 - mae: 2025.1219 - val_loss: 11393499.0000 - val_mae: 2768.9551\n",
      "Epoch 216/350\n",
      "3/3 - 0s - loss: 6562650.0000 - mae: 2026.5818 - val_loss: 11363020.0000 - val_mae: 2770.4944\n",
      "Epoch 217/350\n",
      "3/3 - 0s - loss: 6624415.0000 - mae: 2038.9763 - val_loss: 11374667.0000 - val_mae: 2766.3242\n",
      "Epoch 218/350\n",
      "3/3 - 0s - loss: 6525940.0000 - mae: 2009.9341 - val_loss: 11311744.0000 - val_mae: 2757.9822\n",
      "Epoch 219/350\n",
      "3/3 - 0s - loss: 6760386.0000 - mae: 2055.4412 - val_loss: 11359633.0000 - val_mae: 2760.9719\n",
      "Epoch 220/350\n",
      "3/3 - 0s - loss: 6535204.5000 - mae: 2001.5541 - val_loss: 11344980.0000 - val_mae: 2774.7766\n",
      "Epoch 221/350\n",
      "3/3 - 0s - loss: 6550879.5000 - mae: 1997.9528 - val_loss: 11317229.0000 - val_mae: 2755.2656\n",
      "Epoch 222/350\n",
      "3/3 - 0s - loss: 6539373.0000 - mae: 2025.3091 - val_loss: 11466501.0000 - val_mae: 2769.1121\n",
      "Epoch 223/350\n",
      "3/3 - 0s - loss: 6554027.5000 - mae: 2034.0244 - val_loss: 11385847.0000 - val_mae: 2775.1096\n",
      "Epoch 224/350\n",
      "3/3 - 0s - loss: 6540527.0000 - mae: 1998.7059 - val_loss: 11320957.0000 - val_mae: 2779.4353\n",
      "Epoch 225/350\n",
      "3/3 - 0s - loss: 6586023.5000 - mae: 2023.4653 - val_loss: 11488139.0000 - val_mae: 2782.3435\n",
      "Epoch 226/350\n",
      "3/3 - 0s - loss: 6529498.0000 - mae: 2015.3253 - val_loss: 11440609.0000 - val_mae: 2786.1301\n",
      "Epoch 227/350\n",
      "3/3 - 0s - loss: 6494800.0000 - mae: 2014.7244 - val_loss: 11403468.0000 - val_mae: 2775.1492\n",
      "Epoch 228/350\n",
      "3/3 - 0s - loss: 6621146.0000 - mae: 2002.7500 - val_loss: 11282817.0000 - val_mae: 2775.0618\n",
      "Epoch 229/350\n",
      "3/3 - 0s - loss: 6503463.5000 - mae: 1991.3113 - val_loss: 11400872.0000 - val_mae: 2772.0110\n",
      "Epoch 230/350\n",
      "3/3 - 0s - loss: 6491659.5000 - mae: 2025.2583 - val_loss: 11603105.0000 - val_mae: 2785.1550\n",
      "Epoch 231/350\n",
      "3/3 - 0s - loss: 6626570.0000 - mae: 2047.8485 - val_loss: 11345707.0000 - val_mae: 2775.4553\n",
      "Epoch 232/350\n",
      "3/3 - 0s - loss: 6436307.0000 - mae: 1994.7493 - val_loss: 11300088.0000 - val_mae: 2773.6111\n",
      "Epoch 233/350\n",
      "3/3 - 0s - loss: 6565909.0000 - mae: 1995.6082 - val_loss: 11309437.0000 - val_mae: 2762.9053\n",
      "Epoch 234/350\n",
      "3/3 - 0s - loss: 6413973.0000 - mae: 1996.1930 - val_loss: 11591659.0000 - val_mae: 2773.5703\n",
      "Epoch 235/350\n",
      "3/3 - 0s - loss: 6553935.0000 - mae: 2041.7311 - val_loss: 11516075.0000 - val_mae: 2774.9573\n",
      "Epoch 236/350\n",
      "3/3 - 0s - loss: 6542978.0000 - mae: 2019.1837 - val_loss: 11276453.0000 - val_mae: 2770.4846\n",
      "Epoch 237/350\n",
      "3/3 - 0s - loss: 6459765.5000 - mae: 1990.5271 - val_loss: 11292689.0000 - val_mae: 2766.7383\n",
      "Epoch 238/350\n",
      "3/3 - 0s - loss: 6415475.5000 - mae: 1994.3856 - val_loss: 11326253.0000 - val_mae: 2763.2961\n",
      "Epoch 239/350\n",
      "3/3 - 0s - loss: 6397733.5000 - mae: 1995.9012 - val_loss: 11333883.0000 - val_mae: 2764.5994\n",
      "Epoch 240/350\n",
      "3/3 - 0s - loss: 6439662.5000 - mae: 2000.5447 - val_loss: 11330848.0000 - val_mae: 2765.1829\n",
      "Epoch 241/350\n",
      "3/3 - 0s - loss: 6390751.0000 - mae: 1989.5934 - val_loss: 11359681.0000 - val_mae: 2771.8752\n",
      "Epoch 242/350\n",
      "3/3 - 0s - loss: 6391078.0000 - mae: 1983.1790 - val_loss: 11336453.0000 - val_mae: 2777.8994\n",
      "Epoch 243/350\n",
      "3/3 - 0s - loss: 6363982.0000 - mae: 1978.7446 - val_loss: 11388763.0000 - val_mae: 2781.9053\n",
      "Epoch 244/350\n",
      "3/3 - 0s - loss: 6496531.0000 - mae: 2030.4614 - val_loss: 11347705.0000 - val_mae: 2771.8469\n",
      "Epoch 245/350\n",
      "3/3 - 0s - loss: 6426126.0000 - mae: 1985.0643 - val_loss: 11174651.0000 - val_mae: 2745.5745\n",
      "Epoch 246/350\n",
      "3/3 - 0s - loss: 6402390.0000 - mae: 1983.7378 - val_loss: 11279321.0000 - val_mae: 2759.9668\n",
      "Epoch 247/350\n",
      "3/3 - 0s - loss: 6362207.0000 - mae: 1986.4203 - val_loss: 11309893.0000 - val_mae: 2763.1003\n",
      "Epoch 248/350\n",
      "3/3 - 0s - loss: 6342867.0000 - mae: 1984.7620 - val_loss: 11318315.0000 - val_mae: 2762.8049\n",
      "Epoch 249/350\n",
      "3/3 - 0s - loss: 6488486.5000 - mae: 2024.9080 - val_loss: 11293555.0000 - val_mae: 2754.0413\n",
      "Epoch 250/350\n",
      "3/3 - 0s - loss: 6490274.5000 - mae: 1988.6819 - val_loss: 11197472.0000 - val_mae: 2765.7034\n",
      "Epoch 251/350\n",
      "3/3 - 0s - loss: 6454083.5000 - mae: 1951.5146 - val_loss: 11286443.0000 - val_mae: 2770.1279\n",
      "Epoch 252/350\n",
      "3/3 - 0s - loss: 6323122.0000 - mae: 1990.8928 - val_loss: 11926275.0000 - val_mae: 2814.9424\n",
      "Epoch 253/350\n",
      "3/3 - 0s - loss: 6506409.0000 - mae: 2047.4181 - val_loss: 11417645.0000 - val_mae: 2773.7473\n",
      "Epoch 254/350\n",
      "3/3 - 0s - loss: 6514375.0000 - mae: 1970.0858 - val_loss: 11205137.0000 - val_mae: 2759.1487\n",
      "Epoch 255/350\n",
      "3/3 - 0s - loss: 6505713.0000 - mae: 1965.2728 - val_loss: 11199271.0000 - val_mae: 2743.8533\n",
      "Epoch 256/350\n",
      "3/3 - 0s - loss: 6398089.0000 - mae: 2005.9946 - val_loss: 11470779.0000 - val_mae: 2767.4160\n",
      "Epoch 257/350\n",
      "3/3 - 0s - loss: 6475606.5000 - mae: 2001.3248 - val_loss: 11189709.0000 - val_mae: 2762.3386\n",
      "Epoch 258/350\n",
      "3/3 - 0s - loss: 6326518.0000 - mae: 1960.9889 - val_loss: 11260511.0000 - val_mae: 2763.9111\n",
      "Epoch 259/350\n",
      "3/3 - 0s - loss: 6299296.5000 - mae: 1979.5853 - val_loss: 11471232.0000 - val_mae: 2781.3801\n",
      "Epoch 260/350\n",
      "3/3 - 0s - loss: 6328190.5000 - mae: 1993.7159 - val_loss: 11309659.0000 - val_mae: 2772.1936\n",
      "Epoch 261/350\n",
      "3/3 - 0s - loss: 6304396.5000 - mae: 1968.0333 - val_loss: 11186981.0000 - val_mae: 2763.7810\n",
      "Epoch 262/350\n",
      "3/3 - 0s - loss: 6339830.0000 - mae: 1971.8335 - val_loss: 11293508.0000 - val_mae: 2770.3411\n",
      "Epoch 263/350\n",
      "3/3 - 0s - loss: 6368620.5000 - mae: 1988.1044 - val_loss: 11286704.0000 - val_mae: 2769.6094\n",
      "Epoch 264/350\n",
      "3/3 - 0s - loss: 6294969.5000 - mae: 1961.6229 - val_loss: 11193856.0000 - val_mae: 2775.1204\n",
      "Epoch 265/350\n",
      "3/3 - 0s - loss: 6355153.0000 - mae: 1946.0713 - val_loss: 11215363.0000 - val_mae: 2765.9451\n",
      "Epoch 266/350\n",
      "3/3 - 0s - loss: 6276759.0000 - mae: 1968.8420 - val_loss: 11490805.0000 - val_mae: 2783.2043\n",
      "Epoch 267/350\n",
      "3/3 - 0s - loss: 6337653.5000 - mae: 1999.9966 - val_loss: 11255253.0000 - val_mae: 2762.2620\n",
      "Epoch 268/350\n",
      "3/3 - 0s - loss: 6223504.0000 - mae: 1950.5276 - val_loss: 11166511.0000 - val_mae: 2779.4543\n",
      "Epoch 269/350\n",
      "3/3 - 0s - loss: 6332957.5000 - mae: 1939.7178 - val_loss: 11154185.0000 - val_mae: 2757.3450\n",
      "Epoch 270/350\n",
      "3/3 - 0s - loss: 6237539.0000 - mae: 1949.3783 - val_loss: 11420903.0000 - val_mae: 2772.5486\n",
      "Epoch 271/350\n",
      "3/3 - 0s - loss: 6296628.0000 - mae: 1976.5181 - val_loss: 11454395.0000 - val_mae: 2781.1526\n",
      "Epoch 272/350\n",
      "3/3 - 0s - loss: 6307779.0000 - mae: 1984.5308 - val_loss: 11359971.0000 - val_mae: 2776.7190\n",
      "Epoch 273/350\n",
      "3/3 - 0s - loss: 6230707.5000 - mae: 1962.8459 - val_loss: 11215569.0000 - val_mae: 2758.9583\n",
      "Epoch 274/350\n",
      "3/3 - 0s - loss: 6219435.5000 - mae: 1949.1700 - val_loss: 11157193.0000 - val_mae: 2757.0515\n",
      "Epoch 275/350\n",
      "3/3 - 0s - loss: 6241177.5000 - mae: 1946.8754 - val_loss: 11177751.0000 - val_mae: 2755.8298\n",
      "Epoch 276/350\n",
      "3/3 - 0s - loss: 6209532.0000 - mae: 1958.2611 - val_loss: 11316661.0000 - val_mae: 2766.5422\n",
      "Epoch 277/350\n",
      "3/3 - 0s - loss: 6241546.0000 - mae: 1955.5261 - val_loss: 11183760.0000 - val_mae: 2748.3154\n",
      "Epoch 278/350\n",
      "3/3 - 0s - loss: 6245563.0000 - mae: 1964.0114 - val_loss: 11274399.0000 - val_mae: 2756.8506\n",
      "Epoch 279/350\n",
      "3/3 - 0s - loss: 6220586.0000 - mae: 1947.9460 - val_loss: 11125795.0000 - val_mae: 2747.0469\n",
      "Epoch 280/350\n",
      "3/3 - 0s - loss: 6206882.0000 - mae: 1939.9127 - val_loss: 11131695.0000 - val_mae: 2745.0979\n",
      "Epoch 281/350\n",
      "3/3 - 0s - loss: 6250453.0000 - mae: 1953.6443 - val_loss: 11179787.0000 - val_mae: 2751.2529\n",
      "Epoch 282/350\n",
      "3/3 - 0s - loss: 6232422.5000 - mae: 1961.9412 - val_loss: 11395483.0000 - val_mae: 2760.5264\n",
      "Epoch 283/350\n",
      "3/3 - 0s - loss: 6289728.5000 - mae: 1975.9756 - val_loss: 11139893.0000 - val_mae: 2723.9954\n",
      "Epoch 284/350\n",
      "3/3 - 0s - loss: 6211994.0000 - mae: 1940.4016 - val_loss: 11059108.0000 - val_mae: 2738.5146\n",
      "Epoch 285/350\n",
      "3/3 - 0s - loss: 6268966.0000 - mae: 1947.3429 - val_loss: 11138519.0000 - val_mae: 2746.4075\n",
      "Epoch 286/350\n",
      "3/3 - 0s - loss: 6277217.0000 - mae: 1955.7343 - val_loss: 11359155.0000 - val_mae: 2777.7034\n",
      "Epoch 287/350\n",
      "3/3 - 0s - loss: 6223055.0000 - mae: 1957.0405 - val_loss: 11118695.0000 - val_mae: 2754.8184\n",
      "Epoch 288/350\n",
      "3/3 - 0s - loss: 6199885.5000 - mae: 1941.2457 - val_loss: 11092679.0000 - val_mae: 2736.7444\n",
      "Epoch 289/350\n",
      "3/3 - 0s - loss: 6217446.5000 - mae: 1951.4688 - val_loss: 11125980.0000 - val_mae: 2731.3137\n",
      "Epoch 290/350\n",
      "3/3 - 0s - loss: 6241202.0000 - mae: 1950.1300 - val_loss: 11122027.0000 - val_mae: 2733.1868\n",
      "Epoch 291/350\n",
      "3/3 - 0s - loss: 6170802.0000 - mae: 1926.6047 - val_loss: 11073230.0000 - val_mae: 2746.6287\n",
      "Epoch 292/350\n",
      "3/3 - 0s - loss: 6285395.0000 - mae: 1940.0168 - val_loss: 11186700.0000 - val_mae: 2765.5374\n",
      "Epoch 293/350\n",
      "3/3 - 0s - loss: 6354678.0000 - mae: 2012.8534 - val_loss: 11862624.0000 - val_mae: 2797.3005\n",
      "Epoch 294/350\n",
      "3/3 - 0s - loss: 6228307.0000 - mae: 1998.8843 - val_loss: 11140164.0000 - val_mae: 2758.6152\n",
      "Epoch 295/350\n",
      "3/3 - 0s - loss: 6137427.0000 - mae: 1905.6432 - val_loss: 11150155.0000 - val_mae: 2775.9365\n",
      "Epoch 296/350\n",
      "3/3 - 0s - loss: 6344354.5000 - mae: 1923.4750 - val_loss: 11061819.0000 - val_mae: 2730.3318\n",
      "Epoch 297/350\n",
      "3/3 - 0s - loss: 6220090.5000 - mae: 1935.0536 - val_loss: 11083893.0000 - val_mae: 2721.9260\n",
      "Epoch 298/350\n",
      "3/3 - 0s - loss: 6114465.0000 - mae: 1937.2302 - val_loss: 11334072.0000 - val_mae: 2746.7637\n",
      "Epoch 299/350\n",
      "3/3 - 0s - loss: 6210410.0000 - mae: 1974.5626 - val_loss: 11310656.0000 - val_mae: 2750.5029\n",
      "Epoch 300/350\n",
      "3/3 - 0s - loss: 6146817.5000 - mae: 1951.1185 - val_loss: 11008299.0000 - val_mae: 2739.9023\n",
      "Epoch 301/350\n",
      "3/3 - 0s - loss: 6182646.0000 - mae: 1905.5421 - val_loss: 11038459.0000 - val_mae: 2738.2715\n",
      "Epoch 302/350\n",
      "3/3 - 0s - loss: 6200664.0000 - mae: 1938.7319 - val_loss: 11352527.0000 - val_mae: 2747.3757\n",
      "Epoch 303/350\n",
      "3/3 - 0s - loss: 6260005.5000 - mae: 1955.9286 - val_loss: 11052583.0000 - val_mae: 2720.5120\n",
      "Epoch 304/350\n",
      "3/3 - 0s - loss: 6118051.0000 - mae: 1926.7271 - val_loss: 11079769.0000 - val_mae: 2723.6885\n",
      "Epoch 305/350\n",
      "3/3 - 0s - loss: 6145172.5000 - mae: 1935.7133 - val_loss: 11076533.0000 - val_mae: 2740.3914\n",
      "Epoch 306/350\n",
      "3/3 - 0s - loss: 6146909.0000 - mae: 1916.2415 - val_loss: 11029707.0000 - val_mae: 2738.9060\n",
      "Epoch 307/350\n",
      "3/3 - 0s - loss: 6118405.5000 - mae: 1923.5637 - val_loss: 11127647.0000 - val_mae: 2746.8809\n",
      "Epoch 308/350\n",
      "3/3 - 0s - loss: 6192813.0000 - mae: 1961.5405 - val_loss: 11212999.0000 - val_mae: 2752.4124\n",
      "Epoch 309/350\n",
      "3/3 - 0s - loss: 6357778.5000 - mae: 1946.7119 - val_loss: 10967621.0000 - val_mae: 2725.1062\n",
      "Epoch 310/350\n",
      "3/3 - 0s - loss: 6178673.0000 - mae: 1900.9460 - val_loss: 11189392.0000 - val_mae: 2735.6162\n",
      "Epoch 311/350\n",
      "3/3 - 0s - loss: 6124746.5000 - mae: 1947.8860 - val_loss: 11071482.0000 - val_mae: 2724.6379\n",
      "Epoch 312/350\n",
      "3/3 - 0s - loss: 6106843.5000 - mae: 1927.1847 - val_loss: 10928720.0000 - val_mae: 2717.9844\n",
      "Epoch 313/350\n",
      "3/3 - 0s - loss: 6313145.5000 - mae: 1958.8181 - val_loss: 11132754.0000 - val_mae: 2729.9548\n",
      "Epoch 314/350\n",
      "3/3 - 0s - loss: 6031986.0000 - mae: 1914.3873 - val_loss: 10997751.0000 - val_mae: 2730.1631\n",
      "Epoch 315/350\n",
      "3/3 - 0s - loss: 6113037.5000 - mae: 1900.1648 - val_loss: 11065329.0000 - val_mae: 2753.3577\n",
      "Epoch 316/350\n",
      "3/3 - 0s - loss: 6074980.0000 - mae: 1900.9536 - val_loss: 11170255.0000 - val_mae: 2755.6711\n",
      "Epoch 317/350\n",
      "3/3 - 0s - loss: 6058179.5000 - mae: 1929.2706 - val_loss: 11348141.0000 - val_mae: 2752.3896\n",
      "Epoch 318/350\n",
      "3/3 - 0s - loss: 6126192.5000 - mae: 1937.8430 - val_loss: 10999054.0000 - val_mae: 2721.6565\n",
      "Epoch 319/350\n",
      "3/3 - 0s - loss: 6033766.5000 - mae: 1911.0322 - val_loss: 10971289.0000 - val_mae: 2721.3845\n",
      "Epoch 320/350\n",
      "3/3 - 0s - loss: 6124433.5000 - mae: 1904.1144 - val_loss: 10889241.0000 - val_mae: 2712.4377\n",
      "Epoch 321/350\n",
      "3/3 - 0s - loss: 6196895.0000 - mae: 1944.2466 - val_loss: 11315380.0000 - val_mae: 2745.9072\n",
      "Epoch 322/350\n",
      "3/3 - 0s - loss: 6144567.0000 - mae: 1930.0457 - val_loss: 10912833.0000 - val_mae: 2708.2363\n",
      "Epoch 323/350\n",
      "3/3 - 0s - loss: 6233474.0000 - mae: 1933.2438 - val_loss: 11037447.0000 - val_mae: 2721.6101\n",
      "Epoch 324/350\n",
      "3/3 - 0s - loss: 5986202.0000 - mae: 1899.8864 - val_loss: 10921584.0000 - val_mae: 2720.6826\n",
      "Epoch 325/350\n",
      "3/3 - 0s - loss: 6035199.5000 - mae: 1889.5491 - val_loss: 10897419.0000 - val_mae: 2726.0759\n",
      "Epoch 326/350\n",
      "3/3 - 0s - loss: 6006246.5000 - mae: 1906.7074 - val_loss: 11107964.0000 - val_mae: 2741.7578\n",
      "Epoch 327/350\n",
      "3/3 - 0s - loss: 6058210.5000 - mae: 1928.5828 - val_loss: 11092802.0000 - val_mae: 2728.8105\n",
      "Epoch 328/350\n",
      "3/3 - 0s - loss: 6038455.0000 - mae: 1905.2400 - val_loss: 10842006.0000 - val_mae: 2700.6921\n",
      "Epoch 329/350\n",
      "3/3 - 0s - loss: 6312513.5000 - mae: 1922.6655 - val_loss: 10816096.0000 - val_mae: 2689.7188\n",
      "Epoch 330/350\n",
      "3/3 - 0s - loss: 6273408.5000 - mae: 1964.8400 - val_loss: 11802424.0000 - val_mae: 2782.1379\n",
      "Epoch 331/350\n",
      "3/3 - 0s - loss: 6277435.0000 - mae: 1981.1292 - val_loss: 10875192.0000 - val_mae: 2709.6995\n",
      "Epoch 332/350\n",
      "3/3 - 0s - loss: 6106058.0000 - mae: 1901.0199 - val_loss: 10868895.0000 - val_mae: 2723.5723\n",
      "Epoch 333/350\n",
      "3/3 - 0s - loss: 6077097.0000 - mae: 1898.8187 - val_loss: 10877095.0000 - val_mae: 2734.2429\n",
      "Epoch 334/350\n",
      "3/3 - 0s - loss: 6143813.0000 - mae: 1938.4038 - val_loss: 11169573.0000 - val_mae: 2789.8191\n",
      "Epoch 335/350\n",
      "3/3 - 0s - loss: 6062498.5000 - mae: 1924.9788 - val_loss: 10806643.0000 - val_mae: 2732.2942\n",
      "Epoch 336/350\n",
      "3/3 - 0s - loss: 6054326.0000 - mae: 1910.6173 - val_loss: 10750255.0000 - val_mae: 2720.1582\n",
      "Epoch 337/350\n",
      "3/3 - 0s - loss: 6119383.0000 - mae: 1902.5388 - val_loss: 10951180.0000 - val_mae: 2761.0769\n",
      "Epoch 338/350\n",
      "3/3 - 0s - loss: 6096228.0000 - mae: 1925.8129 - val_loss: 11546744.0000 - val_mae: 2840.1082\n",
      "Epoch 339/350\n",
      "3/3 - 0s - loss: 6447045.5000 - mae: 1991.3185 - val_loss: 10613292.0000 - val_mae: 2654.6709\n",
      "Epoch 340/350\n",
      "3/3 - 0s - loss: 6044720.5000 - mae: 1908.6196 - val_loss: 10808275.0000 - val_mae: 2723.1357\n",
      "Epoch 341/350\n",
      "3/3 - 0s - loss: 5993980.0000 - mae: 1917.4852 - val_loss: 11071122.0000 - val_mae: 2781.4939\n",
      "Epoch 342/350\n",
      "3/3 - 0s - loss: 6040815.0000 - mae: 1936.9872 - val_loss: 11035251.0000 - val_mae: 2784.2158\n",
      "Epoch 343/350\n",
      "3/3 - 0s - loss: 5988896.5000 - mae: 1910.8633 - val_loss: 10689260.0000 - val_mae: 2709.6650\n",
      "Epoch 344/350\n",
      "3/3 - 0s - loss: 5959173.5000 - mae: 1883.3657 - val_loss: 10764775.0000 - val_mae: 2703.1338\n",
      "Epoch 345/350\n",
      "3/3 - 0s - loss: 6341056.0000 - mae: 1971.5649 - val_loss: 9447471.0000 - val_mae: 2398.1731\n",
      "Epoch 346/350\n",
      "3/3 - 0s - loss: 6351014.0000 - mae: 1962.7723 - val_loss: 9082156.0000 - val_mae: 2319.3455\n",
      "Epoch 347/350\n",
      "3/3 - 0s - loss: 6350310.0000 - mae: 1968.1550 - val_loss: 11130895.0000 - val_mae: 2819.4111\n",
      "Epoch 348/350\n",
      "3/3 - 0s - loss: 6354841.5000 - mae: 1976.0262 - val_loss: 11562201.0000 - val_mae: 2912.6426\n",
      "Epoch 349/350\n",
      "3/3 - 0s - loss: 6438877.0000 - mae: 1994.1147 - val_loss: 11578056.0000 - val_mae: 2922.3875\n",
      "Epoch 350/350\n",
      "3/3 - 0s - loss: 6537556.5000 - mae: 1988.9813 - val_loss: 11206736.0000 - val_mae: 2862.5642\n",
      "1/1 - 0s - loss: 11206736.0000 - mae: 2862.5642\n",
      "MSE: 11206736.000, RMSE: 3347.646, MAE: 2862.564\n",
      "Predicted: 16636.326\n"
     ]
    }
   ],
   "source": [
    "# lstm for time series forecasting\n",
    "from numpy import sqrt\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    \n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        \n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        \n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return asarray(X), asarray(y)\n",
    "\n",
    "# load the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-car-sales.csv'\n",
    "df = read_csv(path, header=0, index_col=0, squeeze=True)\n",
    "\n",
    "\n",
    "# retrieve the values\n",
    "values = df.values.astype('float32')\n",
    "\n",
    "\n",
    "# specify the window size\n",
    "n_steps = 5\n",
    "\n",
    "# split into samples\n",
    "X, y = split_sequence(values, n_steps)\n",
    "\n",
    "# reshape into [samples, timesteps, features]\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# split into train/test\n",
    "n_test = 12\n",
    "X_train, X_test, y_train, y_test = X[:-n_test], X[-n_test:], y[:-n_test], y[-n_test:]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', kernel_initializer='he_normal', input_shape=(n_steps,1)))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='Adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=350, batch_size=32, verbose=2, validation_data=(X_test, y_test))\n",
    "\n",
    "# evaluate the model\n",
    "mse, mae = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('MSE: %.3f, RMSE: %.3f, MAE: %.3f' % (mse, sqrt(mse), mae))\n",
    "\n",
    "# make a prediction\n",
    "row = asarray([18024.0, 16722.0, 14385.0, 21342.0, 17180.0]).reshape((1, n_steps, 1))\n",
    "yhat = model.predict(row)\n",
    "print('Predicted: %.3f' % (yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
